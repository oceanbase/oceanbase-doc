# Deploy a single-replica OceanBase cluster

You can deploy a single-replica OceanBase cluster and add nodes to the cluster. OceanBase Database is a piece of single-process software. You only need to install the `oceanbase-xxx.rpm` and `obproxy-xxx.rpm` packages.

This topic describes how to deploy a single-replica OceanBase cluster by using RPM packages.

## Prerequisites

Before you install OceanBase Database, make sure that the following prerequisites are satisfied:

* The OBServer node has been configured. For more information about how to configure an OBServer node, see [Configure servers](../../2.preparations-before-deployment/2.server-configuration.md).

* The deployment environment has been configured. For more information about how to configure the deployment environment, see [Configure the deployment environment](../../4.command-line-deployment/1.configure-a-deployment-environment-command-line/2.configure-deployment-environment-by-using-the-oat-cli.md).

## Procedure

### Step 1: Install the RPM package

1. Install the RPM package for OceanBase Database.

   `$rpm_dir` specifies the directory in which the RPM package is stored. `$rpm_name` specifies the name of the RPM package.

   ```bash
   [root@xxx /]# cd $rpm_dir
   [root@xxx $rpm_dir]# rpm -ivh $rpm_name
   ```

   <main id="notice" type='explain'>
     <h4>Note</h4>
     <p>The OceanBase Database software will be installed under the <code>/home/admin/oceanbase</code> directory. </p>
   </main>

   Example:

   ```bash
   [root@xxx admin]# rpm -ivh oceanbase-*-20220418212020.el7.x86_64.rpm
   Preparing...                          ################################# [100%]
   Updating / installing...
      1:oceanbase-*-20220418212020.################################# [100%]
   ```

2. Install OceanBase Client (OBClient).

   OceanBase Database instances can be in Oracle or MySQL mode. To connect an Oracle tenant to a Java program, the Java driver file (`oceanbase-client-\*.jar`) provided by OceanBase Database is required. To access an Oracle tenant on CLI, you also need to install OBClient.

   OBClient is a CLI client of OceanBase Database. Through OBClient, you can access MySQL and Oracle tenants of OceanBase Database.

   Example:

   ```shell
   [root@xxx $rpm_dir]# rpm -ivh obclient-1.2.6-20210510164331.el7.alios7.x86_64.rpm

   ##Verify whether the installation is successful.##
   [root@xxx $rpm_dir]# which obclient
   /usr/bin/obclient
   ```

### Step 2: Configure directories

1. Clear the OceanBase Database directory (not required at the first deployment).

   If you want to clear the old OceanBase Database environment, or problems occur during the installation and deployment process of OceanBase Database and thereby cause an environmental disorder or generate files that will affect the next installation, you can directly clear the old OceanBase Database directory. `$cluster_name` specifies the cluster name.

   ```shell
   [root@xxx admin]# su - admin
   -bash-4.2$ kill -9 `pidof observer`
   -bash-4.2$ rm -rf /data/1/$cluster_name
   -bash-4.2$ rm -rf /data/log1/$cluster_name
   -bash-4.2$ rm -rf /home/admin/oceanbase/store/$cluster_name /home/admin/oceanbase/log/* /home/admin/oceanbase/etc/*config*
   -bash-4.2$ ps -ef|grep observer
   ```

   Example:

   ```shell
   [root@xxx admin]# su - admin
   -bash-4.2$ kill -9 `pidof observer`
   -bash-4.2$ rm -rf /data/1/obdemo
   -bash-4.2$ rm -rf /data/log1/obdemo
   -bash-4.2$ rm -rf /home/admin/oceanbase/store/obdemo /home/admin/oceanbase/log/* /home/admin/oceanbase/etc/*config*
   -bash-4.2$ ps -ef|grep observer
   ```

2. Initialize the OceanBase Database directory.

   We recommend that you specify the data directory of OceanBase Database to an independent disk and link this directory to the home directory of OceanBase Database by using a soft link. `$cluster_name` specifies the cluster name.

   ```shell
   [root@xxx admin]# su - admin
   -bash-4.2$ mkdir -p /data/1/$cluster_name/{etc3,sort_dir,sstable,slog}
   -bash-4.2$ mkdir -p /data/log1/$cluster_name/{clog,etc2,ilog,oob_clog}
   -bash-4.2$ mkdir -p /home/admin/oceanbase/store/$cluster_name
   -bash-4.2$ for t in {etc3,sort_dir,sstable,slog};do ln -s /data/1/$cluster_name/$t /home/admin/oceanbase/store/$cluster_name/$t; done
   -bash-4.2$ for t in {clog,etc2,ilog,oob_clog};do ln -s /data/log1/$cluster_name/$t /home/admin/oceanbase/store/$cluster_name/$t; done
   ```

   Example:

   ```shell
   [root@xxx admin]# su - admin
   -bash-4.2$ mkdir -p /data/1/obdemo/{etc3,sort_dir,sstable,slog}
   -bash-4.2$ mkdir -p /data/log1/obdemo/{clog,etc2,ilog,oob_clog}
   -bash-4.2$ mkdir -p /home/admin/oceanbase/store/obdemo
   -bash-4.2$ for t in {etc3,sort_dir,sstable,slog};do ln -s /data/1/obdemo/$t /home/admin/oceanbase/store/obdemo/$t; done
   -bash-4.2$ for t in {clog,etc2,ilog,oob_clog};do ln -s /data/log1/obdemo/$t /home/admin/oceanbase/store/obdemo/$t; done
   ```

  <main id="notice" type='explain'>
    <h4>Note</h4>
    <p>The <code>obdemo</code> directory is named after the cluster and can be modified. It is required when the process starts. </p>
  </main>

The following sample code shows how to verify the result:

```shell
-bash-4.2$ cd /home/admin/oceanbase
-bash-4.2$ tree store/
store/
`-- obdemo
 |-- clog -> /data/log1/obdemo/clog
 |-- etc2 -> /data/log1/obdemo/etc2
 |-- etc3 -> /data/1/obdemo/etc3
 |-- ilog -> /data/log1/obdemo/ilog
 |-- oob_clog -> /data/log1/obdemo/oob_clog
 |-- slog -> /data/1/obdemo/slog
 |-- sort_dir -> /data/1/obdemo/sort_dir
 `-- sstable -> /data/1/obdemo/sstable

9 directories, 0 files
```

### Step 3: Initialize the OceanBase cluster

  <main id="notice" type='explain'>
    <h4>Note</h4>
    <p>The IP addresses in the sample code are for reference only. You need to enter the actual server IP address during deployment. </p>
  </main>

1. Start the observer process.

   Example:

   ```shell
   [root@xxx admin]# su - admin
   -bash-4.2$ cd /home/admin/oceanbase && /home/admin/oceanbase/bin/observer -i eth0 -P 2882 -p 2881 -z zone1 -d /home/admin/oceanbase/store/obdemo -r '10.10.10.1:2882:2881' -c 10001 -n obdemo -o "system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/obdemo/etc3;/data/log1/obdemo/etc2"
   devname: eth0
   rpc port: 2882
   mysql port: 2881
   zone: zone1
   data_dir: /home/admin/oceanbase/store/obdemo
   rs list: 10.10.10.1:2882:2881
   cluster id: 10001
   appname: obdemo
   optstr: system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/obdemo/etc3;/data/log1/obdemo/etc2
   ```

   Parameters:

   | **Parameter** | **Description** |
   |--------|----------------------------|
   | `-i` | The NIC name. You can use the `ifconfig` command to view the NIC name.  |
   | `-p` | The service port number, which is usually set to `2881`.  |
   | `-P` | The RPC port number, which is usually set to `2882`.  |
   | `-n` | The name of the cluster. It can be modified. Cluster names must be unique.  |
   | `-z` | The zone where the started observer process belongs.  |
   | `-d` | The primary directory of the cluster. Do not modify other fields except for the cluster name `obdemo`.  |
   | `-c` | The cluster ID. It is a group of digits and can be modified. Cluster IDs must be unique.  |
   | `-l` | The log level.  |
   | `-r` | The RootService list in the format of ip:2882:2881. Multiple items are separated with semicolons (;) to indicate RootService information. This parameter is consistent on the three servers.  |
   | `-o` | The cluster startup parameters that need to be specified as needed. <ul><li>`system_memory` specifies the internal reserved memory of OceanBase Database, which is `30 GB` by default. You can adjust this parameter to a smaller value in the case of insufficient memory on the server. The impact is that memory may be insufficient during performance tests. </li><li>`datafile_size` specifies the size of the `SSTable` data file of OceanBase Database, which is determined based on the available space of `/data/1/`. You no longer need to specify the size again once specified in this step. We recommend that you specify the data file size to at least `100 GB` and reserve some spaces. <blockquote><b>Note</b></br>If `/data/1` and `/data/log1` belong to the same file system (share the same disk), ensure that the space reserved for `/data/log1` is two to four times the size of the memory. </blockquote></li><li>`config_additional_dir` specifies the redundancy directory for the parameter file. </li></ul> |

   You can use the `netstat -ntlp` command to view whether the observer process has started. If ports 2881 and 2882 have been listened to, the observer process has started.

   Example:

   ```shell
   -bash-4.2$ netstat -ntlp
   (Not all processes could be identified, non-owned process info
    will not be shown, you would have to be root to see it all.)
   Active Internet connections (only servers)
   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
   tcp        0      0 0.0.0.0:2881            0.0.0.0:*               LISTEN      79697/observer
   tcp        0      0 0.0.0.0:2882            0.0.0.0:*               LISTEN      79697/observer
   ...
   ```

2. Perform the bootstrap operation on the cluster.

   Connect to the started observer process by using the obclient command. The password is empty.

   ```sql
   [root@xxx admin]# obclient -h127.1 -uroot@sys -P2881 -p
   Enter password:

   obclient> SET SESSION ob_query_timeout=1000000000;
   Query OK, 0 rows affected

   obclient> ALTER SYSTEM BOOTSTRAP ZONE 'zone1' SERVER '10.10.10.1:2882';
   Query OK, 0 rows affected
   ```

  <main id="notice" type='notice'>
    <h4>Notice</h4>
    <p>If an error is reported in this step, the reason may be that the startup parameter of the observer process is incorrect, the privileges on the directories related to the observer process are incorrect, the space of the log directory does not meet the required proportion, or the node memory resources are insufficient. The log directory issue occurs because the log directory shares the same upper-level directory with the data directory, resulting in space occupation by the data directory. Check these problems and then clear the OceanBase Database directory. For more information, see <strong>Clear the OceanBase Database directory (not required at the first deployment)</strong>. </p>
  </main>

3. Verify whether the cluster is initialized.

   After you perform the bootstrap operation and execute the `SHOW DATABASES` statement, if `oceanbase` appears in the database list, the cluster has been initialized.

   ```sql
   obclient> SHOW DATABASES;
   +--------------------+
   | Database           |
   +--------------------+
   | oceanbase          |
   | information_schema |
   | mysql              |
   | SYS                |
   | LBACSYS            |
   | ORAAUDITOR         |
   | test               |
   +--------------------+
   7 rows in set
   ```

4. Change the password.

   By default, the password of the root user under the sys tenant is empty. After successful initialization, you need to change the password.

   ```sql
   obclient> ALTER USER root IDENTIFIED BY '**1***';
   Query OK, 0 rows affected
   ```

## References

* For more information about the cluster startup parameters, see [Overview](../../../../7.reference/5.system-reference/1.system-configuration-items/2.system-configuration-items-overview-list.md).
* For more information about how to change the password of a user, see [ALTER USER](../../../../7.reference/4.development-reference/1.sql-syntax/2.common-tenant-of-mysql-mode/6.sql-statement-of-mysql-mode/18.alter-user-of-mysql-mode.md).
* After the cluster is deployed, you need to create a business tenant. For more information, see [Create a tenant](../5.create-oceanbase-tenant-command-line/2.create-tenant.md).
