# Deploy OceanBase Database in a Kubernetes cluster

This topic describes how to use ob-operator to deploy OceanBase Database in a Kubernetes cluster.

## Background

Like other operators, ob-operator aims to enable OceanBase Database to run in a Kubernetes cluster as a container.
ob-operator now supports the creation and deletion of OceanBase clusters, complete lifecycle management for nodes, and OBProxy management, and will support features such as tenant management and multiple Kubernetes clusters in the future.
You can perform the following steps to use ob-operator to deploy OceanBase Database.

## Deploy CRDs

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/crd.yaml
```

## Deploy ob-operator

### Deploy ob-operator based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/operator.yaml
```

### Deploy ob-operator based on custom configurations

You can modify the `operator.yaml` file based on your configurations.
For example, you can add the startup parameter `--cluster-name` to the `operator.yaml` file. In this way, ob-operator processes only a custom resource definition (CRD) whose `cluster` value is the same as the value of the startup parameter `--cluster-name`.
> **Notice**
>
> The value of `--cluster-name` must be the same as the value of `cluster` in the configurations of the OceanBase cluster.
> Modify the configuration file based on the following content:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    control-plane: controller-manager
  name: ob-operator-controller-manager
  namespace: oceanbase-system
spec:
  replicas: 1
  selector:
    matchLabels:
      control-plane: controller-manager
  template:
    metadata:
      labels:
        control-plane: controller-manager
    spec:
      containers:
      - args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=127.0.0.1:8080
        - --leader-elect
        - --cluster-name=cn
        command:
        - /manager
        image: ob-operator:latest
        imagePullPolicy: Always
        name: manager
```

## Configure labels for Kubernetes nodes

You must configure labels for Kubernetes nodes. The labels must match the value of `nodeSelector` in the `obcluster.yaml` file. ob-operator will schedule pods to nodes with the corresponding labels.
We recommend that you specify the key of a label to `topology.kubernetes.io/zone`. We recommend that you configure different labels for different zones for disaster recovery.

```bash
kubectl label node nodename topology.kubernetes.io/zone=zonename
kubectl label node ${node_name} topology.kubernetes.io/zone=${zone_name}
```

## Deploy local-path-provisioner

In the default configurations of ob-operator, local-path-provisioner is used. Therefore, you must get it ready in advance.

Run the following command to deploy local-path-provisioner:

```text
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
```

For more information, see <https://github.com/rancher/local-path-provisioner>.

## Deploy the OceanBase cluster

### Deploy the OceanBase cluster based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obcluster.yaml
```

### Deploy the OceanBase cluster based on custom configurations

You can modify the `obcluster.yaml` file based on your configurations.
Modify the configuration file based on the following content:

```yaml
apiVersion: cloud.oceanbase.com/v1
kind: OBCluster
metadata:
  name: ob-test
  namespace: obcluster
spec:
  imageRepo: oceanbasedev/oceanbase-cn
  tag: v3.1.3-10100042022051821
  clusterID: 1
  topology:
    - cluster: cn
      zone:
      - name: zone1
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone1
        replicas: 1
      - name: zone2
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone2
        replicas: 1
      - name: zone3
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone3
        replicas: 1
      parameters:
        - name: freeze_trigger_percentage
          value: "30"
  resources:
    cpu: 2
    memory: 10Gi
    storage:
      - name: data-file
        storageClassName: "local-path"
        size: 50Gi
      - name: data-log
        storageClassName: "local-path"
        size: 50Gi
      - name: log
        storageClassName: "local-path"
        size: 30Gi
```

Description of parameters:

- `imageRepo`: the image repository of OceanBase Database.
- `tag`: the tag of the OceanBase image.

- `cluster`: If you want to deploy an OceanBase cluster in a Kubernetes cluster, set this value to the same as that of the startup parameter `--cluster-name` of ob-operator.
- `parameters`: custom parameters of OceanBase Database. Specify this parameter as needed.

- `cpu`: the number of CPU cores. We recommend that you set the value to an integer greater than 2. A value smaller than 2 will cause a system exception.

- `memory`: the memory size. We recommend that you set the value to an integer greater than 10 GiB. A value smaller than 10 GiB will cause a system exception.

- `data-file`: the data file size. The value must be the same as that of the `datafile_size` parameter of the OBServer. We recommend that you set this parameter to a value that is three times the memory size. You can specify `storageClassName` as needed.

- `data-log`: the data directory size. The value must be the same as the size of the `data_dir` directory of the OBServer. We recommend that you set this parameter to a value that is five times the memory size You can specify `storageClassName` as needed.

- `log`: the size of the system log directory of the OBServer. We recommend that you set a value greater than 30 GiB. You can specify `storageClassName` as needed.

## Deploy OBProxy

### Deploy OBProxy based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/service.yaml
```

### Deploy OBProxy based on custom configurations

You can modify the `obproxy.yaml` file based on your configurations.
Modify the configuration file based on the following content:

```yaml
# deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: obproxy
  namespace: obcluster
spec:
  selector:
    matchLabels:
      app: obproxy
  replicas: 3
  template:
    metadata:
      labels:
        app: obproxy
    spec:
      containers:
        - name: obproxy
          image: oceanbase/obproxy-ce:3.2.3
          ports:
            - containerPort: 2883
              name: "sql"
            - containerPort: 2884
              name: "prometheus"
          env:
            - name: APP_NAME
              value: helloworld
            - name: OB_CLUSTER
              value: ob-test
            - name: RS_LIST
              value: $(SVC_OB_TEST_SERVICE_HOST):$(SVC_OB_TEST_SERVICE_PORT)
          resources:
            limits:
              memory: 2Gi
              cpu: "1"
---
# service
apiVersion: v1
kind: Service
metadata:
  name: obproxy-service
  namespace: obcluster
spec:
  type: NodePort
  selector:
    app: obproxy
  ports:
    - name: "tcp"
      port: 2883
      targetPort: 2883
      nodePort: 30083
```

Description of main environment variables:

- `APP_NAME`: the app name of OBProxy.
- `OB_CLUSTER`: the name of the OceanBase cluster to which OBProxy connects.

- `RS_LIST`: the RootService list of the OceanBase cluster, which is generated based on the service address of the OceanBase cluster. After you deploy the OceanBase cluster, two environment variables are automatically generated: `SVC_OB_TEST_SERVICE_HOST` and `SVC_OB_TEST_SERVICE_PORT`. They respectively indicate the service address and port of the OceanBase cluster.

## Deploy Prometheus

### Deploy Prometheus based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role-binding.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/service.yaml
```

### Deploy Prometheus based on custom configurations

The request address used by Prometheus to query OBAgent is configured by using a ConfigMap, and the requests are filtered by service name and port number. If custom configurations have been specified for the OceanBase cluster or OBProxy cluster, you must specify a regular expression for the service name based on the actual situation.

```yaml
# Key configurations in the ConfigMap
scrape_configs:
  - job_name: 'obagent-monitor-basic'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/basic'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-extra'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/extra'
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_pod_container_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-host'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/node/host'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'proxy-monitor'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: obproxy-service;prometheus
      action: keep
```

Specify the service name and port number by using the service discovery capability of Kubernetes. All endpoints will be automatically found. You can query the corresponding monitoring data based on the request path.

Configuration description:

- Service and port:

   - OceanBase Database: ob-operator will create a service based on the name of the deployed OceanBase cluster. The service name is in the format of `svc-monitor-${obcluster_name}`. The port name is `monagent`. If a cluster name has been specified when you deploy OceanBase Database, you must modify the corresponding configuration based on the actual cluster name.
   - OBProxy: You must configure the service and port names for the corresponding OBProxy.

- Request paths for monitoring metrics:

   - Monitoring metrics of OceanBase Database: `/metrics/ob/basic` and `/metrics/ob/extra`

   - Monitoring metrics of OBProxy: `/metrics`. OBProxy supports the exposure of monitoring metrics by using the Prometheus protocol.

### Verify whether the configurations have taken effect

Input the Prometheus address in your browser and press **Enter**. On the page that appears, choose **Status** > **Targets** and verify whether the endpoints under `proxy-monitor`, `obagent-monitor-host`, `obagent-monitor-basic`, and `obagent monitor-extra` are all in the `up` state.
Click **Graph** and enter a PromQL expression for querying the configured parameters.

## Deploy Grafana

### Deploy Grafana based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/pvc.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/service.yaml
```

### Deploy Grafana based on custom configurations

Key configuration information of Grafana, including the address of the Prometheus data source, is set by using a ConfigMap. If Prometheus is deployed based on custom configurations, you must configure the actual service address.

```yaml
# Key configurations in the ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: obcluster
data:
  prometheus.yaml: |-
    {
        "apiVersion": 1,
        "datasources": [
            {
               "access":"proxy",
                "editable": true,
                "name": "prometheus",
                "orgId": 1,
                "type": "prometheus",
                "url": "http://svc-prometheus.obcluster.svc:8080",
                "version": 1,
                "isDefault": true
            }
        ]
    }
```

Configuration description:

- `url`: The service address of Prometheus will be used as the URL of the data source in the configurations of Grafana. You must configure the actual service address of Prometheus.

### Verify whether the configurations have taken effect

Input the Grafana address in your browser and press **Enter**. On the page that appears, input the username and default password `admin` to log on. When you log on for the first time, you will be prompted to change the password.``
Three configured dashboards are available. You can directly import them.

- oceanbase: 15215
- Host: 15216
- obproxy: 15354
   After the import, you can view the corresponding monitoring graphs.

> **Note**
>
> Monitoring data will be displayed only after OBProxy receives actual requests.
