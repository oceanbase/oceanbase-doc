# Experience parallel import and data compression

This topic describes how to use the parallel import and data compression features. 

## Parallel import

In addition to analytical queries, another important part of operational online analytical processing (OLAP) lies in the parallel import of massive amounts of data, namely the batch data processing capability. The parallel execution framework of OceanBase Database allows you to execute data manipulation language (DML) statements in parallel, which is known as parallel DML (PDML) operations. It supports concurrent data writes to multiple servers in a multi-node database without compromising data consistency for large transactions. The asynchronous minor compaction mechanism enhances the support of the LSM-tree-based storage engine for large transactions when memory is tight. 

This topic describes how to experience the PDML feature of OceanBase Database. First, create an empty table named lineitem2 with the same schema as that of the lineitem table used in running the TPC-H benchmark. Then, execute the `insert into ... select` statement to insert 6 million rows of data of the lineitem table into the lineitem2 table. In the following examples, the statement is executed with PDML disabled and enabled respectively for comparison. 

First, copy the schema of the lineitem table and create an empty table named lineitem2. Table partitioning is used to manage large tables in OceanBase Database. In this example, 16 partitions are used. Therefore, the lineitem2 table must also be split into 16 partitions:

```sql
MySQL [test]> show create table lineitem\G;
*************************** 1. row ***************************
       Table: lineitem

Create Table: CREATE TABLE `lineitem` (
  `l_orderkey` bigint(20) NOT NULL,
  `l_partkey` bigint(20) NOT NULL,
  `l_suppkey` bigint(20) NOT NULL,
  `l_linenumber` bigint(20) NOT NULL,
  `l_quantity` bigint(20) NOT NULL,
  `l_extendedprice` bigint(20) NOT NULL,
  `l_discount` bigint(20) NOT NULL,
  `l_tax` bigint(20) NOT NULL,
  `l_returnflag` char(1) DEFAULT NULL,
  `l_linestatus` char(1) DEFAULT NULL,
  `l_shipdate` date NOT NULL,
  `l_commitdate` date DEFAULT NULL,
  `l_receiptdate` date DEFAULT NULL,
  `l_shipinstruct` char(25) DEFAULT NULL,
  `l_shipmode` char(10) DEFAULT NULL,
  `l_comment` varchar(44) DEFAULT NULL,
  PRIMARY KEY (`l_orderkey`, `l_linenumber`),
  KEY `I_L_ORDERKEY` (`l_orderkey`) BLOCK_SIZE 16384 LOCAL,
  KEY `I_L_SHIPDATE` (`l_shipdate`) BLOCK_SIZE 16384 LOCAL
) DEFAULT CHARSET = utf8mb4 ROW_FORMAT = COMPACT COMPRESSION = 'zstd_1.3.8' REPLICA_NUM = 1 BLOCK_SIZE = 16384 USE_BLOOM_FILTER = FALSE TABLET_SIZE = 134217728 PCTFREE = 0 TABLEGROUP = 'x_tpch_tg_lineitem_order_group'
 partition by key(l_orderkey)
(partition p0,
partition p1,
partition p2,
partition p3,
partition p4,
partition p5,
partition p6,
partition p7,
partition p8,
partition p9,
partition p10,
partition p11,
partition p12,
partition p13,
partition p14,
partition p15)
1 row in set
```

### Execute the statement with PDML disabled

PDML is disabled by default. After you create the lineitem2 table, execute the statement with PDML disabled. Because this is a large transaction involving 6 million rows of data, you must increase the default transaction timeout interval (in Î¼s) of OceanBase Database:

```sql
set ob_query_timeout = 1000000000;  
set ob_trx_timeout = 1000000000;  
```

The following results are returned:

```sql
MySQL [test]> insert into lineitem2 select * from lineitem;
Query OK, 6001215 rows affected (1 min 47.312 sec)
Records: 6001215  Duplicates: 0  Warnings: 0
```

![PDML disabled](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer/V3.1.4/zh-CN/quick-start/OceanBase-SQL/3.experience-parallel-import-and-data-compression%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C.png).

You can see that OceanBase Database takes 107s to insert 6 million rows of data in a single transaction when PDML is disabled. 

### Execute the statement with PDML enabled

Add a hint to the statement to enable PDML. Before you execute the statement again, clear the data inserted last time. 

```sql
truncate table lineitem2;
insert /*+ parallel(16) enable_parallel_dml */ into lineitem2 select * from lineitem;
```

Execution results:

```sql
MySQL [test]> truncate table lineitem2;
Query OK, 0 rows affected (0.108 sec)

MySQL [test]> insert /*+ parallel(16) enable_parallel_dml */ into lineitem2 select * from lineitem;
Query OK, 6001215 rows affected (22.117 sec)
Records: 6001215  Duplicates: 0  Warnings: 0
```

![PDML enabled](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer/V3.1.4/zh-CN/quick-start/OceanBase-SQL/3.experience-parallel-import-and-data-compression%E5%B9%B6%E8%A1%8C%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C.png)

You can see that the time required by OceanBase Database to insert 6 million rows of data into the same table is reduced to about 22 seconds when PDML is enabled. After the PDML feature is enabled, the database performance is about 5 times the performance when this feature is disabled. This feature can be useful in scenarios where you want to process data in batches. 

## Data compression

OceanBase Database has its own storage engine based on the LSM-tree structure. Data is roughly classified into baseline data and incremental data. Baseline data is stored in SSTables on the disk, and incremental data is stored in MemTables in the memory. In this way, data can be stored on the disk more compactly. In addition, baseline data on the disk is not frequently updated, and OceanBase Database re-compresses baseline data based on a general compression algorithm. Therefore, data stored in OceanBase Database has a high compression ratio. However, data compression does not degrade the query or write performance. This section describes how to import a large amount of data to OceanBase Database and check the data compression ratio. 

> **Notice**
>
> The encoded compression feature is not released in OceanBase Community Edition V3.1.4. It will be incorporated in later versions. 

### Data preparations

First, use the [CreateData.jar](https://lark-assets-prod-aliyun.oss-accelerate.aliyuncs.com/lark/0/2022/jar/99622/1659402075309-3195cacf-c820-4d3a-98b0-ef1b6f4d2bf5.jar?OSSAccessKeyId=LTAI4GGhPJmQ4HWCmhDAn4F5&Expires=1662013452&Signature=zfgOsOzKSRYayaYkyO8BpQULUFw%3D&response-content-disposition=attachment%3Bfilename*%3DUTF-8%27%27CreateData.jar) tool to generate 50 million rows of test data in the `/home/soft `directory. It takes about 10 minutes to generate the data. You can also use other tools to generate test data. 

```sql
#mkdir /home/soft/
#java -jar CreateData.jar /home/soft/ 50000000
filePath is : /home/soft/
Start Time : 2022-07-10 15:33:00
End Time : 2022-07-10 15:52:53
#du -sh *
10G     t_bigdata_loader.unl
```

OceanBase Database allows you to import data from CSV files in multiple ways. This section describes how to run the load data command to import data. 

1. First, name the generated file and check the actual file size. 

   ```bash
   mv t_bigdata_loader.unl t_f1.csv
   du -sh t_f1.csv
   10G     t_f1.csv
   ```

2. The following example shows the content of the test file `t_f1.csv` generated by the tool by using a random algorithm. The file has eight data columns of different data types. Before you test the data compression feature of OceanBase Database, you must create a table in the tenant and import the records from the CSV file into the table. 

   ```xml
   1|1896404182|1980-06-01|2004-10-25 13:30:39|7470.689|33062564.9527|nOLqnBYtnp|BzWYjZjeodtBNzXSMyBduMNzwDPSiVmhVgPJMeEkeAwKBCorzblwovIHDKBsQhbVjQnIdoeTsiLXTNwyuAcuneuNaol|
   2|572083440|2018-11-09|1998-07-11 01:23:28|6891.054|66028434.4013|UzqteeMaHP|vQWbWBXEWgUqUTzqsOSciiOuvWVcZSrlEOQDwDVGmvGRQYWmhCFdEkpsUsqrWEpKtmxSwURHIHxvmlXHUIxmfelYboeGEuScKKqzpuNLryFsStaFTTRqSsVlCngFFjHnEnpaCnWsdwztbiHJyoGkaxrFmyPAmVregfydArrUZsgRqBpQ|
   3|1139841892|2006-10-07|1999-06-26 17:02:22|286.43692|51306547.5055|KJJtylgxkv|BuBdFTBIIFsEPVxsVBRqAnFXSBdtZDgfumUhIx|
   4|1777342512|1982-12-18|2017-11-19 07:56:35|2986.242|85860387.8696|rTkUBWhdPt|JSazOTAmvtCBrINttDwublNJNRFDIiWkHtWZXmWgKHoZCKGqmmETkIcYLXiSgKkoaATNgjvPxVGjeCOODLEWqrQHqowbMjOLOKrtirWEOpUSxiUudZduTCUvZElKzZfggvCBNthwzKJc|
   ...
   ```

3. Create a table named `t_f1` in the test database of the test tenant. For more information about how to create a tenant, see Manage tenants. 

   ```sql
   obclient -h127.0.0.1 -P2881 -uroot@test  -Dtest -A -p -c

   obclient [test]>create table t_f1(id decimal(10,0),id2 decimal(10,0),id3 date,id4 date,id5 float,id6 float,id7 varchar(30),id8 varchar(300));
   ```

### Import data

Run the built-in load data command of OceanBase Database to import data. The load data command also supports parallel import. Before you import data, complete the following settings. The load data command allows you to import data only from a local OBServer. If you want to import remote data, you can use the OBLOADER tool of OceanBase Database. 

```sql
set ob_query_timeout=1000000000;
set ob_trx_timeout=1000000000;
set GLOBAL secure_file_priv = "";
grant file on *.* to username;
```

> **Notice**
>
> For security reasons, the preceding SQL statement can only be executed locally rather than on a remote OBClient. To be specific, you need to log on to the OBClient (or MySQL client) on the OBServer to execute the statement. 

After you complete the settings, reconnect to the session to make the settings take effect. Then, run the load data command.

```sql
load data /*+ parallel(16) */ infile '/home/soft/t_f1.csv' into table t_f1 fields terminated by '\|' lines terminated by '\n';
```

You can see that OceanBase Database takes about 4 minutes to import 10 GB of data when parallel import is enabled.  In this example, the tenant is configured with 16 CPU cores. You can set an appropriate degree of parallelism (DOP) based on your configurations. A higher DOP indicates faster data import. 

After the data is imported, check the number of records in the table and the space occupied by the table. 

1. Execute the following SQL statement to view the number of table records. In this example, the table contains 50 million records: 

   ```sql
   obclient [test]> select count(*) from t_f1;
   +----------+
   | count(*) |
   +----------+
   | 50000000 |
   +----------+
   ```

2. Trigger a major compaction. 

   To check the compression results of baseline data, log on to your OceanBase database as the administrator of the sys tenant and trigger a major compaction to compact and compress the incremental data and baseline data. You can execute the following SQL statement to trigger a major compaction. 

   ```sql
   obclient -h127.0.0.1 -P2881 -uroot@sys  -Doceanbase -A -p -c
   obclient[oceanbase]> ALTER SYSTEM MAJOR FREEZE;
   ```

3. Execute the following SQL statement to query the status of the major compaction. If the statement returns `IDLE`, the major compaction is completed. 

   ```sql
   MySQL [oceanbase]> select name,info from __all_zone where zone='' and name = 'merge_status';
   +--------------+------+
   | name         | info |
   +--------------+------+
   | merge_status | IDLE |
   +--------------+------+
   1 row in set
   ```

4. Execute the following SQL statement in the sys tenant to query the space occupied by the imported data: 

   ```sql
   obclient [oceanbase]> select b.table_name,svr_ip,role,a.data_size/1024/1024/1024 from __all_virtual_meta_table a,__all_virtual_table b where a.role=1 and a.table_id=b.table_id and b.table_name='T_F1';
   +------------+---------------+------+----------------------------+
   | table_name | svr_ip        | role | a.data_size/1024/1024/1024 |
   +------------+---------------+------+----------------------------+
   | t_f1       | xxx.xx.xxx.xx |    1 |             6.144531250000 |
   +------------+---------------+------+----------------------------+
   ```

The compressed table is about 6.145 GB in size, and the compression ratio (uncompressed size divided by compressed size) is 10/6.145, or 1.62. 
