|description||
|---|---|
|keywords||
|dir-name||
|dir-name-en||
|tenant-type||

# Deploy a standalone OceanBase database by using the CLI

The standalone mode is a streamlined database architecture of OceanBase Database. Compared with a distributed cluster, a standalone database has only one zone that contains only one OBServer node. However, without multiple replicas and scaling capabilities, a standalone database is suitable only for development, testing, and business systems that do not require high data security.

This topic describes how to deploy a standalone OceanBase database by using the CLI.

## Considerations

If you want to implement resource isolation for your OceanBase database, configure cgroups before the deployment.

For more information about resource isolation and cgroup, see [Resource isolation](../../../../600.manage/200.tenant-management/600.common-tenant-operations/300.resource-isolation/100.resource-isolation-overview.md) and [Configure cgroups](../../../../600.manage/200.tenant-management/600.common-tenant-operations/300.resource-isolation/200.resource-isolation-of-oracle-mode/100.config-cgroups-of-oracle-mode.md).

## Prerequisites

Before you install OceanBase Database, make sure that:

* The OBServer node has been configured. For more information, see [Configure servers](../../200.preparations-before-deploy/200.configure-servers.md) and [Initialize an OBServer node by using oatcli](../100.configure-a-deployment-environment-command-line/100.initializing-the-observer-server-using-oatcli.md).

* You have obtained the RPM package of OceanBase Database. For more information, see [Prepare installation packages](../../200.preparations-before-deploy/300.prepare-installation-packages.md).

## Procedure

### Step 1: Install the RPM package

1. Install the RPM package for OceanBase Database.

   ```shell
   [root@xxx /]# cd $rpm_dir
   [root@xxx $rpm_dir]# rpm -ivh $rpm_name
   ```

   Here, `$rpm_dir` specifies the directory in which the RPM package is stored, and `$rpm_name` specifies the name of the RPM package.

   <main id="notice" type='explain'>
      <h4>Note</h4>
      <p>By default, OceanBase Database is installed in the <code>/home/admin/oceanbase</code> directory. </p>
   </main>

   Here is an example:

   ```shell
   [root@xxx /home/admin/rpm]# rpm -ivh oceanbase-4.2.0.0-100000052023073123.el7.x86_64.rpm
   Preparing...                          ################################# [100%]
   Updating / installing...
      1:oceanbase-4.2.0.0-100000052023073################################# [100%]
   ```

2. (Optional) Install OceanBase Client (OBClient).

   OBClient is a CLI tool dedicated to OceanBase Database. You can use it to connect to MySQL tenants and Oracle tenants of OceanBase Database. If you only need to connect to a MySQL tenant, you can also use a MySQL client to access OceanBase Database.

   <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>In order to use versions of OBClient earlier than V2.2.1, install OceanBase Connector/C due to the dependency on OceanBase Connector/C. </br>To obtain the RPM packages of OBClient and OceanBase Connector/C, contact OceanBase Technical Support. </p>
   </main>

   Here is an example:

   ```shell
   [root@xxx /home/admin/rpm]# rpm -ivh obclient-2.2.1-20221122151945.el7.alios7.x86_64.rpm
   Preparing...                          ################################# [100%]
   Updating / installing...
      1:obclient-2.2.1-20221122151945.el7################################# [100%]

   ## Verify that the installation is successful. ##
   [root@xxx /home/admin/rpm]# which obclient
   /usr/bin/obclient
   ```

### Step 2: Configure directories

1. (Optional) Clear the OceanBase Database directory.

   You do not need to clear the directory if you deploy OceanBase Database on the server for the first time.

   You can directly clear the old OceanBase Database directory in the following cases:

   * You want to clear the old OceanBase Database environment.
   * Problems occur during the installation and deployment process of OceanBase Database. The installation environment becomes disordered or files that will affect the next installation are generated.

   ```shell
   [root@xxx /home/admin]# rm -rf /data/1/$cluster_name
   [root@xxx /home/admin]# rm -rf /data/log1/$cluster_name
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/store/$cluster_name
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/log/* /home/admin/oceanbase/etc/*config*
   ```

   Here, `$cluster_name` specifies the cluster name.

   Here is an example:

   ```shell
   [root@xxx /home/admin]# rm -rf /data/1/obdemo
   [root@xxx /home/admin]# rm -rf /data/log1/obdemo
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/store/obdemo
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/log/* /home/admin/oceanbase/etc/*config*
   ```

2. Initialize the OceanBase Database directory.

   We recommend that you specify the data directory of OceanBase Database to an independent disk and link this directory to the home directory of OceanBase Database by using a soft link.

   <main id="notice" type='explain'>
      <h4>Note</h4>
      <p>Starting from V4.3.0, OceanBase Database supports an independent <code>slog</code> disk so that <code>slog</code> files do not need to share a disk with data files. <code>slog</code> files and <code>clog</code> files can share an SSD. For more information about the installation directory of OceanBase Database, see <a href="../../../../700.reference/100.oceanbase-database-concepts/1200.observer-node-architecture/100.observer-installation-directory-structure.md">Structure of the OBServer node installation directory</a>. </p>
   </main>

   ```shell
   ## Switch to the admin user. ##
   [root@xxx /home/admin]# su - admin

   ## Run the following command as the admin user. ##
   -bash-4.2$ mkdir -p /data/1/$cluster_name/{etc3,sstable,slog}
   -bash-4.2$ mkdir -p /data/log1/$cluster_name/{clog,etc2}
   -bash-4.2$ mkdir -p /home/admin/oceanbase/store/$cluster_name
   -bash-4.2$ for t in {etc3,sstable,slog};do ln -s /data/1/$cluster_name/$t /home/admin/oceanbase/store/$cluster_name/$t; done
   -bash-4.2$ for t in {clog,etc2};do ln -s /data/log1/$cluster_name/$t /home/admin/oceanbase/store/$cluster_name/$t; done
   ```

   Here, `$cluster_name` specifies the cluster name.

   Here is an example:

   ```shell
   [root@xxx /home/admin]# su - admin
   -bash-4.2$ mkdir -p /data/1/obdemo/{etc3,sstable,slog}
   -bash-4.2$ mkdir -p /data/log1/obdemo/{clog,etc2}
   -bash-4.2$ mkdir -p /home/admin/oceanbase/store/obdemo
   -bash-4.2$ for t in {etc3,sstable,slog};do ln -s /data/1/obdemo/$t /home/admin/oceanbase/store/obdemo/$t; done
   -bash-4.2$ for t in {clog,etc2};do ln -s /data/log1/obdemo/$t /home/admin/oceanbase/store/obdemo/$t; done
   ```

   <main id="notice" type='explain'>
      <h4>Note</h4>
      <p>The <code>obdemo</code> directory is named after the cluster and can be modified. It is required when the process starts. </p>
   </main>

   The result is as follows:

   ```shell
   -bash-4.2$ cd /home/admin/oceanbase
   -bash-4.2$ tree store/
   store/
   `-- obdemo
      |-- clog -> /data/log1/obdemo/clog
      |-- etc2 -> /data/log1/obdemo/etc2
      |-- etc3 -> /data/1/obdemo/etc3
      |-- slog -> /data/1/obdemo/slog
      `-- sstable -> /data/1/obdemo/sstable

   6 directories, 0 files
   ```

### Step 3: Initialize OceanBase Database

<main id="notice" type='explain'>
   <h4>Note</h4>
   <p>The IP addresses in the sample code are for reference only. You need to enter the actual server IP address during deployment. </p>
</main>

1. Start the observer process.

   <main id="notice" type='notice'>
     <h4>Notice</h4>
     <p>You must start the observer process as the <code>admin</code> user. </p>
   </main>

   OceanBase Database allows you to start the observer process on an OBServer node with a specified IPv4 or IPv6 address.

   **Start the observer process on an OBServer node with an IPv4 address:**

   ```shell
   cd /home/admin/oceanbase && /home/admin/oceanbase/bin/observer {-I $ip | -i $devname} -P $rpc_port -p $sql_port -z $zone_name -d /home/admin/oceanbase/store/$cluster_name -r '$ip:2882:2881' -c $cluster_id -n $cluster_name -o "system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/$cluster_name/etc3;/data/log1/$cluster_name/etc2"
   ```

   **Start the observer process on an OBServer node with an IPv6 address:**

   ```shell
   cd /home/admin/oceanbase && /home/admin/oceanbase/bin/observer -6 {-I $ip | -i $devname} -P $rpc_port -p $sql_port -z $zone_name -d /home/admin/oceanbase/store/$cluster_name -r '[$ip]:2882:2881' -c $cluster_id -n $cluster_name -o "system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/$cluster_name/etc3;/data/log1/$cluster_name/etc2"
   ```

   The parameters are described in the following table.

   | **Parameter** | **Description** |
   |--------|----------------------------|
   | `-6` | This parameter is required when you start the observer process on an OBServer node with an IPv6 address.  |
   | `-I` \| `-i` | <ul><li>`-I`: the IP address of the node to be started. In multi-node deployment, you cannot use 127.0.0.1 as the destination IP address. We recommend that you use an IP address such as `-I 10.10.10.1`, to start a node. </li><li>`-i`: the NIC name. You can use the `ifconfig` command to view the NIC name. </li></ul><main id="notice" type='explain'><h4>Note</h4><p>OceanBase Database allows you to start a node by specifying both the IP address and the NIC. For example, `-I 10.10.10.1 -i eth0`. However, we recommend that you do not use this method. </p></main> |
   | `-p` | The service port number, which is usually set to `2881`.  |
   | `-P` | The RPC port number, which is usually set to `2882`.  |
   | `-n` | The name of the cluster. It can be modified. Cluster names must be unique.  |
   | `-z` | The zone to which the started observer process belongs.  |
   | `-d` | The primary directory of the cluster, which is created during initialization. Do not modify fields other than `$cluster_name`.  |
   | `-c` | The cluster ID. It is a group of digits and can be modified. Cluster IDs must be unique.  |
   | `-l` | The log level.  |
   | `-r` | The RootService list in the format of `$ip:2882:2881`. Multiple items are separated with semicolons (;) to indicate RootService information. <main id="notice" type='notice'><h4>Notice</h4><p>When you start the observer process on an OBServer node with an IPv6 address, you must enclose the IPv6 address with square brackets (<code>[]</code>). </p></main> |
   | `-o` | Optional. The cluster startup parameters. You can specify values for multiple parameters and separate the settings of different parameters with commas (,). We recommend that you set appropriate values for cluster startup parameters to optimize cluster performance and resource utilization. Here are some commonly used cluster startup parameters:<ul><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/3700.cpu_count.md">cpu_count</a>: the total number of system CPU cores. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/22400.system_memory.md">system_memory</a>: the memory reserved for the tenant whose ID is <code>500</code>, that is, the internal reserved memory of OceanBase Database. If the machine has a small memory size, you can set this parameter to a smaller value. However, insufficient memory may occur during performance testing. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/13500.memory_limit.md">memory_limit</a>: the total memory size available. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/4700.datafile_size.md">datafile_size</a>: the size of disk space available for data files. That is, the size of the data file <code>sstable</code> (for one-time initialization) in OceanBase Database. You can evaluate the value of this parameter based on the available space on <code>/data/1/</code>. We recommend that the value is no less than <code>100G</code>. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/4400.datafile_disk_percentage.md">datafile_disk_percentage</a>: the percentage of total disk space allowed for data files. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/4600.datafile_next.md">datafile_next</a>: the step size of automatic scale-out for disk files. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/4500.datafile_maxsize.md">datafile_maxsize</a>: the maximum size of disk space allowed for automatic scale-out of data files. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/3600.config_additional_dir.md">config_additional_dir</a>: the local directories for storing multiple copies of configuration files for redundancy. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/12700.log_disk_size.md">log_disk_size</a>: the size of the log disk where REDO logs are stored. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/12600.log_disk_percentage.md">log_disk_percentage</a>: the percentage of total disk space occupied by REDO logs. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/22200.syslog_level.md">syslog_level</a>: the level of system logs. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/22100.syslog_io_bandwidth_limit.md">syslog_io_bandwidth_limit</a>: the maximum I/O bandwidth available for system logs. If this value is reached, the remaining system logs are discarded. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/13300.max_syslog_file_count.md">max_syslog_file_count</a>: the maximum number of log files that can be retained. </li><li> <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/9000.enable_syslog_recycle.md">enable_syslog_recycle</a>: specifies whether to enable system log recycling. </li></ul> You can configure <code>datafile_size</code>, <code>datafile_disk_percentage</code>, <code>datafile_next</code>, and <code>datafile_maxsize</code> together to achieve automatic scale-out of disk space for data files. For more information, see <a href="../../../../700.reference/200.system-management/1000.disk-data-file-management/100.disk-data-file-dynamic-expansion.md">Configure automatic scale-out of disk space for data files</a>. For more information about cluster configuration, see <a href="../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/200.system-configuration-items-overview-list.md">Cluster-level parameters</a>.  |

   Here is an example of starting the observer process on an OBServer node with an IPv4 address:

   ```shell
   [root@xxx /home/admin]# su - admin
   -bash-4.2$ cd /home/admin/oceanbase && /home/admin/oceanbase/bin/observer -I 10.10.10.1 -P 2882 -p 2881 -z zone1 -d /home/admin/oceanbase/store/obdemo -r '10.10.10.1:2882:2881' -c 10001 -n obdemo -o "system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/obdemo/etc3;/data/log1/obdemo/etc2"
   ```

   You can use the following commands to check whether the observer process has started successfully:

   * Run the `netstat -ntlp` command. If ports `2881` and `2882` are listened to, the observer process is started.
   * Run the `ps -ef|grep observer` command to return information about the observer process.

   Here is an example:

   ```shell
   -bash-4.2$ netstat -ntlp
   (Not all processes could be identified, non-owned process info
   will not be shown, you would have to be root to see it all.)
   Active Internet connections (only servers)
   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
   tcp        0      0 0.0.0.0:2881            0.0.0.0:*               LISTEN      11111/observer
   tcp        0      0 0.0.0.0:2882            0.0.0.0:*               LISTEN      11111/observer
   ...        ...    ...                       ...                     ...         ...

   -bash-4.2$ ps -ef|grep observer
   admin    11111      0 43 17:58 ?        00:00:14 /home/admin/oceanbase/bin/observer -I 10.10.10.1 -P 2882 -p 2881 -z zone1 -d /home/admin/oceanbase/store/obdemo -r 10.10.10.1:2882:2881 -c 10001 -n obdemo -o system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/obdemo/etc3;/data/log1/obdemo/etc2
   ```

   Here is an example of starting the observer process on an OBServer node with an IPv6 address:

   ```shell
   [root@xxx /home/admin]# su - admin
   ```

   ```shell

   -bash-4.2$ cd /home/admin/oceanbase && /home/admin/oceanbase/bin/observer -6 -I xxxx:xxxx:xxxx:xxxx:xxx:xxxx:xxxx:ebd8 -P 2882 -p 2881 -z zone1 -d /home/admin/oceanbase/store/obdemo -r '[xxxx:xxxx:xxxx:xxxx:xxx:xxxx:xxxx:ebd8]:2882:2881' -c 10001 -n obdemo -o "system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/obdemo/etc3;/data/log1/obdemo/etc2"
   ```

2. Perform the bootstrap operation on the cluster.

   1. Use OBClient to connect to the started observer process. The password is empty.

      ```shell
      [root@xxx /home/admin]# obclient -h127.0.0.1 -uroot -P2881 -p******
      ```

   2. Set the maximum execution duration allowed for an SQL query.

      ```sql
      obclient [(none)]> SET SESSION ob_query_timeout=1000000000;
      ```

   3. Specify a RootService server list and start the cluster.

      * Use an IPv4 address to specify a RootService server list and start the cluster.

         ```sql
         obclient [(none)]> ALTER SYSTEM BOOTSTRAP ZONE 'zone1' SERVER '10.10.10.1:2882';
         ```

      * Use an IPv6 address to specify a RootService server list and start the cluster.

         ```sql
         obclient [(none)]> ALTER SYSTEM BOOTSTRAP ZONE 'zone1' SERVER '[xxxx:xxxx:xxxx:xxxx:xxx:xxxx:xxxx:ebd8]:2882';
         ```

   <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>If an error is reported in this step, the reason may be that the startup parameter of the observer process is incorrect, the privileges on the directories related to the OBServer node are incorrect, the space of the log directory does not meet the required proportion, or the node memory resources are insufficient. The log directory issue occurs because the log directory shares the same upper-level directory with the data directory, resulting in space occupation by the data directory. Check these issues and then clear the OceanBase Database directory. </p>
   </main>

3. Verify that the cluster is initialized.

   After you perform the bootstrap operation and execute the `SHOW DATABASES` statement, if `oceanbase` appears in the database list, the cluster is initialized.

   ```shell
   obclient [(none)]> SHOW DATABASES;
   +--------------------+
   | Database           |
   +--------------------+
   | information_schema |
   | LBACSYS            |
   | mysql              |
   | oceanbase          |
   | ORAAUDITOR         |
   | SYS                |
   | test               |
   +--------------------+
   7 rows in set
   ```

4. Change the password.

   By default, the password of the root user under the `sys` tenant is empty. After successful initialization, you need to change the password.

   ```shell
   obclient [(none)]> ALTER USER root IDENTIFIED BY '******';
   Query OK, 0 rows affected
   ```

## What to do next

After the cluster is created, you can create user tenants based on your business needs.

For more information about how to create a tenant by using the CLI, see [Create a tenant](../../../../600.manage/200.tenant-management/600.common-tenant-operations/200.manage-create-tenant.md).

## References

* [Connect to an OceanBase tenant by using OBClient](../../../../300.develop/100.application-development-of-mysql-mode/100.connect-to-oceanbase-database-of-mysql-mode/300.connect-to-an-oceanbase-tenant-by-using-obclient-of-mysql-mode.md)
* [System parameters](../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/200.system-configuration-items-overview-list.md)
* [ALTER USER](../../../../700.reference/500.sql-reference/100.sql-syntax/200.common-tenant-of-mysql-mode/600.sql-statement-of-mysql-mode/1800.alter-user-of-mysql-mode.md)
* [User tenants](../../../../600.manage/200.tenant-management/400.introduction-of-user-tenant.md)
* [CREATE TENANT](../../../../700.reference/500.sql-reference/100.sql-syntax/100.system-tenants/800.create-tenant.md)
* [Online upgrade from standalone deployment to distributed deployment](../../../../600.manage/100.cluster-management/300.common-cluster-operations/1400.single-machine-deployment-online-to-distributed.md)