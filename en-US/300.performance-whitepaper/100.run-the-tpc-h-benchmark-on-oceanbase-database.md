Run the TPC-H benchmark on OceanBase Database 
==================================================================



Introduction to TPC-H 
------------------------------------------

TPC-H is a business intelligence test set developed by the Transaction Processing Performance Council (TPC) of the United States to simulate the decision-making process of applications. It is commonly used in academia and industry for evaluating the performance of decision support applications. This BI benchmark comprehensively evaluates the overall business computing capabilities of decision support systems and imposes high requirements on vendors of such systems. Due to its universal and practical business value, TPC-H is widely used in credit analysis and credit card analysis of banks, telecom operation analysis, tax analysis, and tobacco industry decision analysis. 

The TPC-H benchmark is the successor of TPC-D, a benchmark developed by TPC in 1994 for decision support systems. TPC-H implements a data warehouse in the Third Normal Form (3NF) that contains eight basic relationships. The main evaluation metric is the response time of each query from its submission to the return of results. The TPC-H benchmark measures the number of queries executed per hour (QphH@size) in a database. H indicates the average number of complex queries executed per hour and size indicates the scale of the database. This metric reflects the query processing capacity of a database system. The TPC-H benchmark is modeled based on real-world production and operation environments. This enables it to evaluate some key performance metrics that cannot be evaluated by other benchmarks. The TPC-H benchmark fills the gap in data warehouse testing and encourages database vendors and research institutions to push the decision support technology to its limit.

Prepare the environment 
--------------------------------------------

Before testing, prepare the test environment as per the following requirements:

* JDK: Use V1.8u131 and later

  

* Make: Run the \`yum install make\` command to install Make

  

* GCC: Run the \`yum install gcc\` command to install GCC

  

* MySQL-devel: Run the \`yum install mysql-devel\` command to install MySQL-devel

  

* MySQL Connector/Python: Run the \`sudo yum install MySQL-python\` command to install MySQL Connector/Python

  

* PrettyTable: Run the \`pip install prettytable\` command to install PrettyTable

  

* JDBC:Use MySQL Connector/J 5.1.47

  

* TPC-H tools: Download from the [TPC-H Tools Download](https://www.tpc.org/tpc_documents_current_versions/download_programs/tools-download-request5.asp?bm_type=TPC-H&bm_vers=3.0.0&mode=CURRENT-ONLY) page

  

* OBClient: For more information, see [OBClient Documentation](https://github.com/oceanbase/obclient/blob/master/README.md).

  

* OceanBase Database: For more information, see [Quick Start](../2.quick-start/1.overview-of-quick-start.md).

  

* Tenant specifications

  ```unknow
  Create resource unit tpch_unit max_cpu 26, max_memory 60000000000, max_iops 128, max_disk_size 53687091200, max_session_num 64, MIN_CPU=26, MIN_MEMORY=60000000000, MIN_IOPS=128;
  create resource pool tpch_pool unit = 'tpch_unit', unit_num = 1, zone_list=('zone1','zone2','zone3');
  create tenant tpch_mysql resource_pool_list=('tpch_pool'), charset=utf8mb4, replica_num=3, zone_list('zone1', 'zone2', 'zone3'), primary_zone=RANDOM, locality='F@zone1,F@zone2,F@zone3' set variables ob_compatibility_mode='mysql', ob_tcp_invited_nodes='%';
  ```

  
  **Notice**

  
  * The preceding tenant specifications are based on [OceanBase TPC-H benchmark report](../3.performance-whitepaper/2.tpc-h-benchmark-report-of-oceanbase-database.md). You need to adjust the specifications based on your hardware configuration. For more information, see [Modify a tenant](../6.administrator-guide/3.basic-database-management/5.tenant-management/5.modify-a-tenant.md).

    
  
  * To test the performance of a tenant, you need to configure the maximum available resources of the tenant.

    
  

  
  




Deploy a cluster 
-------------------------------------

1. Use OBD to deploy OceanBase clusters. 

   Deploy the cluster in the 1-1-1 architecture, where the cluster has three zones and each zone has an OBServer.
   

2. Create a new tenant and user to run the TPC-H benchmark. Do not use the sys tenant to run the benchmark. Set the value of the tenant's `primary_zone` parameter to `RANDOM`. For more information about tenant settings, see [Create a user tenant](../6.administrator-guide/3.basic-database-management/5.tenant-management/2.create-a-user-tenant.md).

   The value `RANDOM` indicates that the leader of the new table partitions is randomly assigned to one of the three servers.
   




Tune the environment 
-----------------------------------------

You need to tune the environment in a sys tenant. 

1. Tune the OceanBase database. 

   Run the following command in the sys tenant: `obclient -h$host_ip -P$host_port -uroot@sys -A`.

   ```unknow
   # Adjust the memory usage of the sys tenant to provide more resources for the test tenant. You need to adjust the memory usage based on the actual environment.
   alter system set system_memory='15g';
   alter resource unit sys_unit_config max_memory='15g',min_memory='15g';
   # Database tuning parameters
   alter system set enable_merge_by_turn= False;
   alter system set trace_log_slow_query_watermark='100s';
   alter system set max_kept_major_version_number=1;
   alter system set enable_sql_operator_dump=True;
   alter system set _hash_area_size='3g';
   alter system set memstore_limit_percentage=50;
   alter system set enable_rebalance=False;
   alter system set memory_chunk_cache_size='0';
   alter system set minor_freeze_times=5;
   alter system set merge_thread_count=20;
   alter system set cache_wash_threshold='30g';
   ## Adjust the log level and the maximum number of log files that can be saved.
   alter system set syslog_level='PERF';
   alter system set max_syslog_file_count=100;
   alter system set enable_syslog_recycle='True';
   ```

   

2. Set the tenant parameters. 

   Run the following command in the test user: `obclient -h$host_ip -P$host_port -u$user@$tenant -p$password -A`. 

   ```unknow
   # Set global parameters.
   set global ob_sql_work_area_percentage=80;
   set global optimizer_use_sql_plan_baselines = true;
   set global optimizer_capture_sql_plan_baselines = true;
   alter system set ob_enable_batched_multi_statement='true';
   ## Set the tenant to prevent transaction timeout.
   set global ob_query_timeout=36000000000;
   set global ob_trx_timeout=36000000000;
   set global max_allowed_packet=67108864;
   set global secure_file_priv="";
   /*
   We recommend that you set the value of the parallel_max_servers parameter to 10 times the number of CPU cores allocated to the resource unit of the test tenant.
   For example, if the resource unit of the test tenant is set to: create resource unit $unit_name max_cpu 26,
   then set the value of the parallel_max_servers parameter to 260.
   We also recommend that you set the value of the parallel_server_target parameter to the value of the parallel_max_servers parameter * the number of servers * 0.8.
   So, the value is 260 * 3 * 0.8 = 624
   */
   set global parallel_max_servers=260;
   set global parallel_servers_target=624;
   ```

   

3. After you configure the tuning parameters, restart the cluster by running the following command: `obd cluster restart $cluster_name --wop`.

   Here, `wop` indicates that you restart the cluster without parameters. If the OBServer in the cluster is started for the first time, this option does not work.
   




Run the TPC-H benchmark 
--------------------------------------------

### Install TPC-H tools 

Download TPC-H tools. For more information, visit the [TPC-H Tools Download](https://www.tpc.org/tpc_documents_current_versions/download_programs/tools-download-request5.asp?bm_type=TPC-H&bm_vers=3.0.0&mode=CURRENT-ONLY) page. 

Unzip the package and enter the TPC-H directory. 

```unknow
[wieck@localhost ~] $ unzip 7e965ead-8844-4efa-a275-34e35f8ab89b-tpc-h-tool.zip
[wieck@localhost ~] $ cd TPC-H_Tools_v3.0.0
```



Replicate the `makefile.suite` file. 

```unknow
[wieck@localhost TPC-H_Tools_v3.0.0] $ cd dbgen/
[wieck@localhost dbgen] $ cp Makefile.suite Makefile
```



Redefine the CC, DATABASE, MACHINE, and WORKLOAD parameters in the `makefile.suite` file. 

```unknow
CC      = gcc
# Current values for DATABASE are: INFORMIX, DB2, TDAT (Teradata)
#                                  SQLSERVER, SYBASE, ORACLE, VECTORWISE
# Current values for MACHINE are:  ATT, DOS, HP, IBM, ICL, MVS,
#                                  SGI, SUN, U2200, VMS, LINUX, WIN32
# Current values for WORKLOAD are:  TPCH
DATABASE= MYSQL
MACHINE = LINUX
WORKLOAD = TPCH
```



Add new macro definition to the `tpcd.h` file. 

```unknow
#ifdef MYSQL
#define GEN_QUERY_PLAN ""
#define START_TRAN "START TRANSACTION"
#define END_TRAN "COMMIT"
#define SET_OUTPUT ""
#define SET_ROWCOUNT "limit %d;\n"
#define SET_DBASE "use %s;\n"
#endif
```



Compile the file. 

```unknow
make
```



### Generate data 

You can generate 10 GB, 100 GB, or 1 TB of data as needed. The following example shows the command to generate 100 GB of data. 

```unknow
./dbgen -s 100
mkdir tpch100
mv *.tbl tpch100
```



### Generate the SQL statement. 

Copy the `qgen` and `dists.dss` files to the `queries` directory. 

```unknow
cp qgen queries
cp dists.dss queries
```



In the `queries` directory, create a `gen.sh` script to generate the SQL query. 

```unknow
#!/usr/bin/bash
for i in {1..22}
do  
  ./qgen -d $i -s 100 > db"$i".sql
done
```



Run the `gen.sh` script. 

```unknow
chmod +x  gen.sh
./gen.sh
```



Modify the SQL queries. 

```unknow
dos2unix *
```



In the generated file, delete the `limit -xx` clause because OceanBase Database does not support a limit of negative numbers, delete the (3) after the day, and add `parallel(96)` to specify the execution concurrency. The following example shows the statement of a sample SQL query q1:

```unknow
SELECT  /*+    parallel(96) */   --- Specify the execution concurrency.
    l_returnflag,
    l_linestatus,
    sum(l_quantity) as sum_qty,
    sum(l_extendedprice) as sum_base_price,
    sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
    sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
    avg(l_quantity) as avg_qty,
    avg(l_extendedprice) as avg_price,
    avg(l_discount) as avg_disc,
    count(*) as count_order
from
    lineitem
where
    l_shipdate <= date'1998-12-01' - interval '90' day (3) --- Delete (3).
group by
    l_returnflag,
    l_linestatus
order by
    l_returnflag,
    l_linestatus;
limit -1;              --- Delete this line.
```



### Create a table 

Create the schema file `create_tpch_mysql_table_part.ddl`. 

```sql
Create tablegroup if not exists tpch_tg_100g_lineitem_order_group binding true partition by key 1 partitions 192;
create tablegroup if not exists tpch_tg_100g_partsupp_part binding true partition by key 1 partitions 192;

drop database if exists $db_name;
create database $db_name;
use $db_name;

    create table lineitem (
    l_orderkey bigint not null,
    l_partkey bigint not null,
    l_suppkey bigint not null,
    l_linenumber bigint not null,
    l_quantity bigint not null,
    l_extendedprice bigint not null,
    l_discount bigint not null,
    l_tax bigint not null,
    l_returnflag char(1) default null,
    l_linestatus char(1) default null,
    l_shipdate date not null,
    l_commitdate date default null,
    l_receiptdate date default null,
    l_shipinstruct char(25) default null,
    l_shipmode char(10) default null,
    l_comment varchar(44) default null,
    primary key(l_orderkey, l_linenumber))
    tablegroup = tpch_tg_100g_lineitem_order_group
    partition by key (l_orderkey) partitions 192;
    create index I_L_ORDERKEY on lineitem(l_orderkey) local;
    create index I_L_SHIPDATE on lineitem(l_shipdate) local;

    create table orders (
    o_orderkey bigint not null,
    o_custkey bigint not null,
    o_orderstatus char(1) default null,
    o_totalprice bigint default null,
    o_orderdate date not null,
    o_orderpriority char(15) default null,
    o_clerk char(15) default null,
    o_shippriority bigint default null,
    o_comment varchar(79) default null,
    primary key (o_orderkey))
    tablegroup = tpch_tg_100g_lineitem_order_group
    partition by key(o_orderkey) partitions 192;
    create index I_O_ORDERDATE on orders(o_orderdate) local;


    create table partsupp (
    ps_partkey bigint not null,
    ps_suppkey bigint not null,
    ps_availqty bigint default null,
    ps_supplycost bigint default null,
    ps_comment varchar(199) default null,
    primary key (ps_partkey, ps_suppkey))
    tablegroup tpch_tg_100g_partsupp_part
    partition by key(ps_partkey) partitions 192;


    create table part (
  p_partkey bigint not null,
  p_name varchar(55) default null,
  p_mfgr char(25) default null,
  p_brand char(10) default null,
  p_type varchar(25) default null,
  p_size bigint default null,
  p_container char(10) default null,
  p_retailprice bigint default null,
  p_comment varchar(23) default null,
  primary key (p_partkey))
  tablegroup tpch_tg_100g_partsupp_part
  partition by key(p_partkey) partitions 192;


    create table customer (
  c_custkey bigint not null,
  c_name varchar(25) default null,
  c_address varchar(40) default null,
  c_nationkey bigint default null,
  c_phone char(15) default null,
  c_acctbal bigint default null,
  c_mktsegment char(10) default null,
  c_comment varchar(117) default null,
  primary key (c_custkey))
  partition by key(c_custkey) partitions 192;

  create table supplier (
  s_suppkey bigint not null,
  s_name char(25) default null,
  s_address varchar(40) default null,
  s_nationkey bigint default null,
  s_phone char(15) default null,
  s_acctbal bigint default null,
  s_comment varchar(101) default null,
  primary key (s_suppkey)
) partition by key(s_suppkey) partitions 192;


    create table nation (
  n_nationkey bigint not null,
  n_name char(25) default null,
  n_regionkey bigint default null,
  n_comment varchar(152) default null,
  primary key (n_nationkey));

    create table region (
  r_regionkey bigint not null,
  r_name char(25) default null,
  r_comment varchar(152) default null,
  primary key (r_regionkey));
```



### Load data 

You can write scripts based on the data and SQL queries generated in the preceding steps. As an example, you can take the following steps to load data:

1. Create a directory for loading scripts. 

   ```unknow
   mkdir load
   cd load
   cp ../dss.ri  ../dss.ddl ./
   ```

   

2. Create the `load.py` script. 

   ```unknow
   $cat load.py
   #/usr/bin/evn python
   #-*- encoding:utf-8 -*-
   import os
   import sys
   import time
   import commands
   hostname='$host_ip'  # Be careful. You need to specify the IP address of an OBServer, such as  OBServer A.
   port='$host_port'               # The port number of OBServer A.
   tenant='$tenant_name'              # Specify the tenant name.
   user='$user'               # Specify the username.
   password='$password'           # Specify the password.
   data_path='$path'         # Be careful. You need to specify the directory of tbl on the OBServer A.
   db_name='$db_name'             # Specify the database name.
   # Create a table.
   cmd_str='obclient -h%s -P%s -u%s@%s -p%s < create_tpch_mysql_table_part.ddl'%(hostname,port,user,tenant,password)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str='obclient -h%s -P%s -u%s@%s -p%s  -D%s -e "show tables;" '%(hostname,port,user,tenant,password,db_name)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s -c  -D%s -e "load data /*+ parallel(80) */ infile '%s/customer.tbl' into table customer fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s -c  -D%s -e "load data /*+ parallel(80) */ infile '%s/lineitem.tbl' into table lineitem fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s -c -D%s -e "load data /*+ parallel(80) */ infile '%s/nation.tbl' into table nation fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s -c  -D%s -e "load data /*+ parallel(80) */ infile '%s/orders.tbl' into table orders fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s   -D%s -e "load data /*+ parallel(80) */ infile '%s/partsupp.tbl' into table partsupp fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s -c  -D%s -e "load data /*+ parallel(80) */ infile '%s/part.tbl' into table part fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s -c  -D%s -e "load data /*+ parallel(80) */ infile '%s/region.tbl' into table region fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   cmd_str=""" obclient -h%s -P%s -u%s@%s -p%s -c  -D%s -e "load data /*+ parallel(80) */ infile '%s/supplier.tbl' into table supplier fields terminated by '|';" """ %(hostname,port,user,tenant,password,db_name,data_path)
   result = commands.getstatusoutput(cmd_str)
   print result
   ```

   

3. Load the data. 

   **Notice**

   
   To load data, you need to install the OBClient.

   ```unknow
   $python load.py
   (0,'')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.\nTABLE_NAME\nT1\nLINEITEM\nORDERS\nPARTSUPP\nPART\nCUSTOMER\nSUPPLIER\nNATION\nREGION')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.')
   (0, 'obclient: [Warning] Using a password on the command line interface can be insecure.')
   ```

   

4. Perform a major compaction. 

   A major compaction compacts SSTables and MEMTables of the current major version with the full static data of the previous major version and generates a new set of full data. This makes the statistics of the storage layer more accurate and improves the stability of execution plans. 
   **Notice**

   
   To perform a major compaction, you need to log on as the sys tenant.

   ```unknow
   MySQL [(none)]> use oceanbase
   Database changed
   MySQL [oceanbase]> alter system major freeze;
   Query OK, 0 rows affected
   ```

   

5. Check whether the compaction is completed. 

   ```unknow
   MySQL [oceanbase]> select name,value from oceanbase.__all_zone where name='frozen_version' or name='last_merged_version';
   +---------------------+-------+
   | name                | value |
   +---------------------+-------+
   | frozen_version      |     2 |
   | last_merged_version |     2 |
   | last_merged_version |     2 |
   | last_merged_version |     2 |
   | last_merged_version |     2 |
   +---------------------+-------+
   ```

   

   The compaction task is completed when the value of `frozen_version` equals that of `last_merged_version` of all zones.
   




### Run the test 

You can write scripts based on the data and SQL queries generated in the preceding steps. As an example, you can take the following steps to perform the test:

1. Write the `tpch.sh` script in the `queries` directory. 

   ```unknow
   #!/bin/bash
   TPCH_TEST="obclient -h $host_ip -P $host_port -utpch_100g_part@tpch_mysql  -D tpch_100g_part  -ptest -c"
   # Run a warmup.
   for i in {1..22}
   do
       sql1="source db${i}.sql"
       echo $sql1| $TPCH_TEST >db${i}.log  || ret=1
   done
   # Run the formal test.
   for i in {1..22}
   do
       starttime=`date +%s%N`
       echo `date  '+[%Y-%m-%d %H:%M:%S]'` "BEGIN Q${i}"
       sql1="source db${i}.sql"
       echo $sql1| $TPCH_TEST >db${i}.log  || ret=1
       stoptime=`date +%s%N`
       costtime=`echo $stoptime $starttime | awk '{printf "%0.2f\n", ($1 - $2) / 1000000000}'`
       echo `date  '+[%Y-%m-%d %H:%M:%S]'` "END,COST ${costtime}s"
   done
   ```

   

2. Run the test script. 

   ```unknow
   sh tpch.sh
   ```

   



**Note**



For more information about the test result, see [OceanBase TPC-H benchmark report](../3.performance-whitepaper/2.tpc-h-benchmark-report-of-oceanbase-database.md).

FAQ 
------------------------

* Q. An error occurred while importing the data. Error message:

  ```unknow
  ERROR 1017 (HY000) at line 1: File not exist
  ```

  

  The tbl file must be placed in a directory on the server that hosts OceanBase Database, because only local data can be imported and loaded.
  

* Q. An error occurred when I was view the data. Error message:

  ```unknow
  ERROR 4624 (HY000): No memory or reach tenant memory limit
  ```

  

  A. You are running out of memory. Allocate more memory to the tenant.
  

* Q. An error occurred while importing data. Error message:

  ```unknow
  ERROR 1227 (42501) at line 1: Access denied
  ```

  

  You need to grant the access permission to the logon user. Run the following command:

  ```unknow
  grant file on *.* to tpch_100g_part;
  ```

  




