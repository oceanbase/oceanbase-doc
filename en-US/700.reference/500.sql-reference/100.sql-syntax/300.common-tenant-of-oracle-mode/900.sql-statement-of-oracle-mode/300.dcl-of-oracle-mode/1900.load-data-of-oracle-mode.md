| Description   |                 |
|---------------|-----------------|
| keywords      |                 |
| dir-name      |                 |
| dir-name-en   |                 |
| tenant-type   | Oracle Mode     |

# LOAD DATA

## Purpose

You can use this statement to import data from an external file.

In OceanBase Database, the `LOAD DATA` statement can load input files from OSS file systems and OBServer nodes.

<main id="notice" type='explain'>
  <h4>Note</h4>
  <p><ul><li>The current version of OceanBase Database does not support the <code>LOCAL INFILE</code> statement. </li><li>Do not use the <code>LOAD DATA</code> statement on tables with triggers. </li></ul></p>
</main>

You can use the `LOAD DATA` statement to import a CSV text file in the following process:

1. Parse the file: OceanBase Database reads data from a file based on the file name that you enter and determines whether to perform parallel or serial parsing of data from the input file based on the specified degree of parallelism (DOP).

2. Distribute the data: OceanBase Database is a distributed database. Data of each partition may be distributed across different OBServer nodes. The `LOAD DATA` statement is used to process the parsed data and determine to which OBServer node the data is sent.

3. Insert the data: After the destination OBServer node receives the data, it executes the `INSERT` statement to insert the data into the corresponding partition.

To import data from an external file, you must have the `FILE` privilege. You can use the `GRANT FILE ON *.* TO $user_name;` statement to grant the privilege, where `$user_name` is the user that executes the `LOAD DATA` statement.

## Syntax

```sql
LOAD DATA
    [ [/*+ PARALLEL(N) load_batch_size(M) APPEND */]
      | [/*+ PARALLEL(N) direct(bool, int) */] ]
    [REMOTE_OSS] INFILE 'file_name'
    INTO TABLE table_name
    [{FIELDS | COLUMNS}
        [TERMINATED BY 'string']
        [[OPTIONALLY] ENCLOSED BY 'char']
        [ESCAPED BY 'char']
    ]
    [LINES
        [STARTING BY 'string']
        [TERMINATED BY 'string']
    ]
    [IGNORE number {LINES | ROWS}]
    [(column_name_var
        [, column_name_var] ...)]
```

## Parameters

| Parameter | Description |
|----------------------------------|---------------------------------|
| parallel(N) | The DOP for loading data. The default value of `N` is `4`.  |
| load_batch_size(M) | The batch size of each insertion. The default value of `M` is `100`. Recommended value range: [100, 1000].  |
| APPEND | A hint for enabling the bypass import feature. This feature allows you to allocate space and write data to data files. By default, this hint is equivalent to `direct(false, 0)` and can collect statistics online like the `GATHER_OPTIMIZER_STATISTICS` hint does.  |
| direct | A hint for enabling the bypass import feature. The `bool` parameter in `direct(bool, int)` specifies whether the given CSV file needs to be ordered. The value `true` indicates that the file must be ordered. The `int` parameter specifies the maximum number of error rows allowed.  |
| REMOTE_OSS | Specifies whether to read data from an OSS file system. This parameter is optional. <main id="notice" type='notice'><h4>Notice</h4><p>If this parameter is specified, <code>file_name</code> must be the address of an OSS service. </p></main> |
| file_name | The path and file name of the input file.  <code>file_name</code> can be in one of the following formats:<ul><li>For an input file on an OBServer node: <code>/\$PATH/\$FILENAME</code> </li><li>For an input file in an OSS file system: <code>oss://\$PATH/\$FILENAME/?host=\$HOST&access_id=\$ACCESS_ID&access_key=\$ACCESSKEY</code> </li></ul>     The parameters are described as follows:<ul><li><code>\$PATH</code>: the file path in the bucket, which represents the directory where the file is located. </li><li><code>\$FILENAME</code>: the name of the file to be accessed. </li><li><code>\$HOST</code>: the host name or CDN domain name of the OSS service, that is, the address of the OSS service to be accessed. </li><li><code>\$ACCESS_ID</code>: the AccessKey ID required for authentication when accessing the OSS service. </li><li><code>\$ACCESSKEY</code>: the AccessKey secret required for authentication when accessing the OSS service. </li></ul>    <main id="notice" type='explain'> <h4>Note</h4> <p>When you import a file from OSS, make sure that:<ul><li>You have privileges to access the corresponding OSS bucket and file. That is, you can read data from the bucket and file. Usually, you need to set access privileges on the OSS console or through OSS APIs, and configure AccessKey information (AccessKey ID and AccessKey secret) as credentials with corresponding privileges. </li><li>The database server can connect to the specified <code>$HOST</code> address to access the OSS service. The CDN is configured correctly and the network connection is normal if you want to use the CDN domain name to access the OSS service. </li></ul></p> </main> |
| table_name | The name of the table from which data is imported. Partitioned and non-partitioned tables are supported.  |
| FIELDS \| COLUMNS | The format of the field.  <ul><li> `ENCLOSED BY`: specifies the modifier of the exported value.   </li> <li> `TERMINATED BY`: specifies the end character of the exported column.  </li> <li>`ESCAPED BY`: specifies the characters ignored for the exported value. </li></ul> |
| LINES STARTING BY | The start character of the line.  |
| LINES TERMINATED BY | The end character of the line.  |
| IGNORE number { LINES \| ROWS } | Specifies to ignore the first few lines. `LINES` indicates the first few lines of the file. `ROWS` indicates the first few rows of data specified by the field delimiter. By default, fields in the input file are mapped to columns in the destination table one by one. If the input file does not contain all the columns, the missing columns are filled based on the following mappings: <ul><li> Character data type: empty string    </li> <li> Numeric data type: 0     </li> <li> Date data type: `0000-00-00` </li></ul> |
| column_name_var | The name of the imported column.  |

## Examples

### Example 1: Import data from a file on an OBServer node

1. Set a global security path.

   <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>For security reasons, when you set the system variable <code>secure_file_priv</code>, you can connect to the database only through a local socket to execute the SQL statement that modifies the global variable. For more information, see <a href="../../../../../800.configuration-items-and-system-variables/200.system-variable/300.global-system-variable/11500.secure_file_priv-global.md">secure_file_priv</a>. </p>
   </main>

   ```shell
   obclient> SET GLOBAL secure_file_priv = "/";
   ```

2. Log out of the database.

   <main id="notice" type='explain'>
     <h4>Note</h4>
     <p>Because <code>secure_file_priv</code> is a <code>GLOBAL</code> variable, you need to run <code>\q</code> to exit for the settings to take effect. </p>
   </main>

   ```shell
   obclient> \q
   ```

   The return result is as follows:

   ```shell
   Bye
   ```

3. Connect to the database again. Execute the `LOAD DATA` statement to import data.

   * Perform normal import.

      ```shell
      obclient> LOAD DATA INFILE '/home/admin/test.csv' INTO TABLE t1;
      ```

   * Use the `APPEND` hint to enable bypass import.

      ```shell
      LOAD DATA /*+ PARALLEL(4) APPEND */ INFILE '/home/admin/test.csv' INTO TABLE t1;
      ```

### Example 2: Import data from an OSS file

Use the `direct(bool,int)` hint to enable bypass import. The import file can be an OSS file.

```shell
LOAD DATA /*+ direct(true,1024) parallel(16) */ REMOTE_OSS INFILE 'oss://antsys-oceanbasebackup/backup_rd/xiaotao.ht/lineitem2.tbl?host=***.oss-cdn.***&access_id=***&access_key=***' INTO TABLE tbl1 FIELDS TERMINATED BY ',';
```

## References

* For more examples of the `LOAD DATA` statement, see [Import data by using the LOAD DATA statement](../../../../../../500.data-migration/700.migrate-data-from-csv-file-to-oceanbase-database/200.use-the-load-command-to-load-the-csv-data-file-to-the-oceanbase-database.md).
* For more bypass import examples of the `LOAD DATA` statement, see [Import data in bypass mode by using the LOAD DATA statement](../../../../../../500.data-migration/1100.bypass-import/200.use-load-data-statement-to-bypass-import-data.md).
