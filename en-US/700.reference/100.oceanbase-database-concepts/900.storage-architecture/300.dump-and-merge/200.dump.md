| description ||
|---|---|
| keywords ||
| dir-name ||
| dir-name-en ||
| tenant-type ||

# Minor compaction

The storage engine of OceanBase Database uses the log-structured merge-tree (LSM-tree) architecture. In this architecture, data is stored in the MemTable and SSTable. When the memory occupied by the MemTable exceeds the specified threshold, data in the MemTable is flushed to the SSTable to release the memory space. This process is called a minor compaction.

## Layered minor compaction

In OceanBase Database V2.1 and earlier, only one SSTable is generated by a minor compaction at a time. When the MemTable requires a minor compaction, the data in the MemTable and the generated SSTable is merged. As more minor compactions are performed, the SSTable becomes increasingly large in size, and the amount of data in a single minor compaction also increases. This slows down minor compaction efficiency and thereby causes the memory of the MemTable to be exhausted. Starting from V2.2, OceanBase Database introduced the hierarchical minor compaction strategy.

![Minor compactions](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.1/700.reference/100.oceanbase-database-concepts/900.storage-architecture/300.dump-and-merge/dump.png)

Based on specific implementations in the industry and the architecture of OceanBase Database, you can regard hierarchical minor compaction as a tiered-leveled compaction solution. The L0 layer is where size-tiered compactions are performed and split into multiple sublayers based on scenarios. Leveled compactions are performed at the L1 and L2 layers based on the granularity of macroblocks.

### L0 layer

The L0 layer is where mini SSTables are placed. The SSTables at the L0 layer may be empty based on the parameter settings of different minor compaction strategies. For the L0 layer, server-level parameters are provided to specify the number of sublayers and the maximum number of SSTables allowed per sublayer. The sublayers of the L0 layer are numbered from level-0 to level-n, and the maximum number of SSTables allowed is the same for each sublayer. If the number of SSTables at the level-n sublayer reaches the upper limit, these SSTables are compacted into one SSTable and written to the level-n+1 sublayer. If the number of SSTables at the lowest sublayer of the L0 layer reaches the upper limit, a compaction from L0 to L1 is performed to release the memory space. If the L0 layer exists, the frozen MemTables are compacted to generate a new mini SSTable for the level-0 sublayer of the L0 layer. The multiple SSTables at each sublayer of L0 are sorted by base_version. The versions of SSTables involved in subsequent intra-layer or inter-layer major compactions must be adjacent. In this way, the SSTables are arranged in order by version, which simplifies the operation logic of subsequent reads and major compactions.

The internal layers of L0 slow down the compaction to L1 and reduce write amplification but cause read amplification. For example, L0 contains n sublayers and m SSTables per sublayer. L0 contains at least (n Ã— m + 2) SSTables, including minor SSTables and major SSTables. Therefore, the number of sublayers and the maximum number of SSTables allowed per sublayer must be controlled within a reasonable range.

### L1 layer

The L1 layer is where minor SSTables are placed. The minor SSTables at the L1 layer are sorted in order by rowkey. When the number of mini SSTables at the L0 layer reaches the threshold, the minor SSTables are involved in the compactions at the L0 layer. L1-layer compactions are scheduled only when the ratio of the total size of mini SSTables at the L0 layer to that of minor SSTables at the L1 layer reaches a specified threshold. Otherwise, compactions are performed only within the L0 layer. This improves the compaction efficiency and reduces the overall write amplification.

### L2 layer

The L2 layer is where the baseline major SSTable is placed. The major SSTable is read-only and does not participate in actual compaction operations during routine minor compactions. This ensures that the baseline data is consistent among the replicas.

## Minor compaction triggering

A minor compaction can be automatically or manually triggered.

When a tenant's active MemStore memory usage reaches `freeze_trigger_percentage * memstore_limit (where memstore_limit = tenant memory * memstore_limit_percentage)`, it automatically triggers a freeze (the preparation for a minor compaction), after which the system internally schedules a minor compaction.

You can also run the following commands to manually trigger a minor compaction.

<main id="notice" type='explain'>
    <h4>Note</h4>
    <p><code>memstore_limit_percentage</code> specifies the percentage of the memory that can be occupied by MemStores to the total available memory of a tenant. For more information about this parameter, see <a href="../../../800.configuration-items-and-system-variables/100.system-configuration-items/300.cluster-level-configuration-items/13800.memstore_limit_percentage.md">memstore_limit_percentage</a>. </p>
  </main>

Here are some examples:

* Cluster-level minor compaction

   * Initiate a minor compaction for the `sys` tenant.

      ```sql
      obclient> ALTER SYSTEM MINOR FREEZE TENANT = sys;
      ```

   * Initiate a minor compaction for all user tenants.

      ```sql
      obclient> ALTER SYSTEM MINOR FREEZE TENANT = all_user;
      ```

   * Initiate a minor compaction for all meta tenants.

      ```sql
      obclient> ALTER SYSTEM MINOR FREEZE TENANT = all_meta;
      ```

* Tenant-level minor compaction

   ```sql
   obclient> ALTER SYSTEM MINOR FREEZE TENANT= prod_tenant;
   ```

## References

For more information about minor compactions, see [Minor compactions](../../../200.system-management/500.manage-data-storage/100.dump-management/100.dump-management-overview.md).