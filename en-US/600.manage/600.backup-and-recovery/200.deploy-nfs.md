| description ||
|---|---|
| keywords ||
| dir-name ||
| dir-name-en ||
| tenant-type ||

# Deploy NFS

We recommend that you use Alibaba Cloud Object Storage Service (OSS) as the backup destination. OSS is stateless and more stable than the stateful Network File System Version 4 (NFS4).

To use NFS as the backup destination, deploy NFS first by referring to this topic. NFS is available in software and hardware. Software NFS is unstable and tends to hang. As a result, backup tasks may be stuck or the server to which NFS mounts cannot work normally. Therefore, we recommend that you use a dedicated NFS hardware device.

## Considerations

* To use an NFS environment, make sure that you have mounted NFS before enabling the backup service. If NFS fails during backup, stop data backup and log archiving first and then resolve the NFS issue.

* Concurrent backup control in OceanBase Database depends on the file lock feature of NFS version 4. Therefore, we recommend that you mount NFS version 4.1 or later.

* When you use NFS to back up data and logs, ensure that all OBServer nodes are mounted to NFS of the same server and use the parameter settings recommended in this topic. For more information about the procedure of mounting NFS, see **Deploy the NFS client**.

* When you restart an OBServer node, you need to start NFS first.

* After you add a new server, make sure that you have mounted NFS or that you can back up data to another destination before you start the new OBServer node.

## Deploy the NFS server

<main id="notice" type='notice'>
<h4>Notice</h4>
<p>If you use an NFS hardware device, skip this operation and directly deploy the NFS client. </p>
</main>

1. Log on to the NFS server.

2. Run the following command to install NFS by using the YUM package manager:

   ```shell
   sudo yum install nfs-utils
   ```

3. Configure the exports file.

   1. Select a directory as the shared directory. Before you select the directory, ensure that the directory size meets the backup requirements for space and performance.

      The selected shared directory in the following sample code is `/data/nfs_server/`.

   2. Run the `sudo vim /etc/exports` command to open the configuration file. Then, set the following information:

      ```shell
      /data/nfs_server/ xx.xx.xx.xx/16(rw,sync,all_squash)
      ```

      Here, `xx.xx.xx.xx` specifies the accessible CIDR block.

   3. Run the following command to grant privileges to `nfsnobody` to allow `nfsnobody` to access the directory specified in the `exports` file:

      ```shell
      sudo chown nfsnobody:nfsnobody -R /data/nfs_server
      ```

4. Set the NFS parameters.

   1. Run the `sudo vim /etc/sysconfig/nfs` command to open the configuration file.

   2. Set the following parameters:

      ```shell
      RPCNFSDCOUNT=8
      RPCNFSDARGS="-N 2 -N 3 -U"
      NFSD_V4_GRACE=90
      NFSD_V4_LEASE=90
      ```

   3. Run the following commands to restart NFS:

      ```shell
      sudo systemctl restart nfs-config

      sudo systemctl restart nfs-server
      ```

5. Set the slot table.

   1. Run the `sudo vim /etc/sysctl.conf` command to open the `sysctl.conf` file. Then, add the following string to the file:

      ```xml
      sunrpc.tcp_max_slot_table_entries=128
      ```

   2. Run the following command to change the maximum number of concurrent NFS requests to `128`:

      ```sql
      sudo sysctl -w sunrpc.tcp_max_slot_table_entries=128
      ```

      After the command is executed, you can run the `cat /proc/sys/sunrpc/tcp_max_slot_table_entries` command to check whether the settings have taken effect. If the return value is `128`, the modification is successful.

   3. (Optional) Restart the server.

## Deploy the NFS client

The NFS client must be deployed on all OBServer nodes.

This section describes how to deploy the NFS client on an OBServer node.

1. Log on to an OBServer node.

2. Run the following command to install NFS by using the YUM package manager:

   ```shell
   sudo yum install nfs-utils
   ```

3. Set the slot table.

   1. Run the `sudo vim /etc/sysctl.conf` command to open the `sysctl.conf` file. Then, add the following string to the file:

      ```xml
      sunrpc.tcp_max_slot_table_entries=128
      ```

   2. Run the following command to change the maximum number of concurrent NFS requests to `128`:

      ```sql
      sudo sysctl -w sunrpc.tcp_max_slot_table_entries=128
      ```

      After the command is executed, you can run the `cat /proc/sys/sunrpc/tcp_max_slot_table_entries` command to check whether the settings have taken effect. If the return value is `128`, the modification is successful.

   3. (Optional) Restart the server.

4. Select a directory as the mount point and run the following command to mount NFS.

   In the following sample command, NFS is mounted to the `/data/nfs` directory. You can create a directory if no appropriate mounting directory is available.

   ```shell
   sudo mount -tnfs4 -o rw,nfsvers=4.1,sync,lookupcache=positive,hard,timeo=600,wsize=1048576,rsize=1048576,namlen=255 10.10.10.1:/data/nfs_server /data/nfs
   ```

   where:

   * `nfsvers=4.1` specifies the NFS version. We recommend that you use NFS version 4.1 or later because the backup depends on the native file lock of NFS version 4. NFS version 4.0 has a bug, namely, the old file may be read after it is renamed.

   * `sync` indicates that synchronous write is adopted, which ensures that data can be promptly synchronized to the server, thereby ensuring data consistency.

   * `lookupcache=positive` prevents the system to prompt that the accessed directory or file does not exist during concurrent access, thereby ensuring data consistency.

   * `hard` indicates that the system will block the read and write requests from applications when the NFS is unavailable, thereby ensuring data consistency. Do not specify the `soft` option because it may cause data errors.

   * `timeo` specifies the time to wait for a retry. The unit is 0.1s. We recommend that you do not set it to a large value. The recommended value is `600`.

   * `wsize` specifies the size of the blocks to write. The recommended value is `1048576`.

   * `rsize` specifies the size of the blocks to read. The recommended value is `1048576`.

   * `namlen` specifies the name length. The recommended value is `255`.

   * `10.10.10.1` specifies the IP address of the NFS server.

   <main id="notice" type='notice'>
   <h4>Notice</h4>
   <ul>
   <li>When you mount the NFS, make ensure that the following parameters are set for the backup mount environment: <code>nfsvers=4.1</code>, <code>sync</code>, <code>lookupcache=positive</code>, and <code>hard</code>. </li>
   <li>In a Docker container, the NFS must be mounted to the destination host and then mapped to the Docker container. If the NFS is directly mounted in the Docker container, the NFS client may be hung. </li>
   </ul>
   </main>

5. Run the following command to verify the performance of NFS:

   ```shell
   fio -filename=/data/nfs/fio_test -direct=1  -rw=randwrite -bs=2048K -size=100G  -runtime=300 -group_reporting -name=mytest  -ioengine=libaio -numjobs=1 -iodepth=64 -iodepth_batch=8 -iodepth_low=8 -iodepth_batch_complete=8
   ```

   The result is as follows:

   ```shell
   Run status group 0 (all jobs):
   WRITE: io=322240MB, aggrb=1074.2MB/s, minb=1074.2MB/s, maxb=1074.2MB/s, mint=300006msec, maxt=300006msec
   ```