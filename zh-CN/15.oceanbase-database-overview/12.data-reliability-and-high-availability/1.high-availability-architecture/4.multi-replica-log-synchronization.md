多副本日志同步 
============================



日志服务作为关系数据库的基础组件，在 OceanBase 数据库中的重要性主要体现在如下方面： 

* 通过在事务提交时持久化 Memtable Mutator 的内容以及事务状态信息，为事务的原子性（Atomic）和持久性（Durability）提供支持。

  

* 通过生成 trans_version，并通过 Keepalive 消息同步到所有备机（以及只读副本），为事务的隔离性（Isolation）提供支持。

  

* 通过实现 Paxos 协议将日志在多数派副本上实现强同步，为分布式数据库的数据容灾以及高可用提供支持；并进而支持各类副本类型（只读副本、日志副本等）。

  

* 通过维护权威的成员组和 Leader 信息，为 OceanBase 数据库中的各个模块所使用。为 RootService 的负载均衡、轮转合并等复杂策略提供底层机制。

  

* 提供外部数据服务，为 OMS 和增量备份等外部工具提供增量数据。

  




OceanBase 数据库日志服务的挑战 
-----------------------------------------

与传统关系型数据库的日志模块相比，OceanBase 数据库的日志服务主要有如下几点挑战：

* 通过 Multi-Paxos 替换传统的主备同步机制，进而实现系统的高可用和数据的高可靠。需要支持全球部署，多个副本之间的网络时延达到几十甚至数百 ms。

  

* 作为分布式数据库，OceanBase 数据库以 Partition 作为数据同步的基本单位。单机要求支持十万量级的 Partition，在此基础上，需要支持日志数据的高效写入和读取。同时，任何功能本身均需要考虑操作的批量化，以加速执行速度。

  

* 日志服务维护副本的成员列表、Leader 等状态信息，需要为分布式系统的丰富功能提供高效的支持。

  




整体结构 
-------------------------

![日志](https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/0273623461/p353011.jpg)

Paxos Core 
-------------------------------

Paxos Core 是日志服务的核心。日志服务实现了一套标准的 Multi-Paxos 协议，保证在未发生多数派永久性故障的前提下，所有提交成功的数据均可恢复。

Paxos Core 实现了乱序日志提交，保证事务之间无依赖，同时对异地部署非常友好，避免单个网络链路故障影响后续所有事务的 RT。

为确保 Multi-Paxos 的正确性，日志服务提供了多层次的 Checksum 校验：

* 单条日志的 Checksum 校验。

  

* Group Commit 后 Block 整体的 Checksum 校验。

  

* RPC packet 的 Checksum 校验。

  

* log_id 的累积 Checksum 校验，每一条日志都校验当前 Partition 之前所有日志的正确性。

  




正确、高效的协议实现、不间断运行的容灾切换测试用例以及实时完备的 Checksum 校验，一同确保日志服务的正确性。

成员组管理 
--------------------------

日志服务维护着每个 Partition 的成员组信息。成员组信息包括当前 Paxos Group 的 member_list 以及 Quorum 信息。

支持的功能如下：

* add_member/remove_member， 支持加减成员。负载均衡发起的迁移、宕机引起的补副本操作，均需要通过上述功能修改 Partition 的成员组。

  

* modify_quorum，修改 Paxos Group 的法定副本数。执行 Locality 变更（例如， 3F-\>5F），地域级故障降级容灾（5F-\>3F），均需要通过上述功能修改 Paxos Group 的法定副本数。

  




上述功能均提供了批量操作接口，允许将多个 Partition 的操作批量执行，极大的提升了相关操作的执行效率。

如通过 batch_modify_quorum 接口，单机 1 万个分区 5F-\>3F 的降级容灾操作执行时间由 20 分钟优化到只需要 20s，极大的提升了系统在故障场景下的容灾能力和响应时间。

有主改选 
-------------------------

OceanBase 数据库作为使用 LSM-Tree 实现的数据库系统，需要周期性的将 Memtable 的内容转储或合并到磁盘，在执行合并时需要消耗大量 CPU。为了避免合并时影响正常的业务请求，也为了加快合并速度，OceanBase 数据库通过轮转合并，在执行合并前将 Leader 切换到同 Region 的另外一个副本。

日志服务为有主改选提供支持。有主改选执行分两步：

1. RS 查询每个待改选 Partition 的有效 candidates。

   

2. RS 根据 candidates list 以及 Region、Primary Zone 等信息，确定改选目标，下发命令，执行改选请求。

   




故障恢复 
-------------------------

OceanBase 数据库在线上实际运行过程中难免遇到各类故障（磁盘、网络故障，机器异常宕机等）。作为高可用的分布式数据库系统，必须能够在各种故障场景应对自如，保证 RPO = 0，RTO 尽量短。这里按照不同的故障类型分别讲述日志服务在其中所起到的作用：

1. Leader 节点以外的其他节点出现少数派宕机：由于 Paxos 要求日志只需要同步到 Majority 即可，对可用性无影响，RPO=0，RTO=0。

   

2. 包括 Leader 节点在内出现少数派宕机：通过无主选举+Paxos 恢复流程，保证在 lease 过期后很短的时间内即选出新的 Leader 提供服务，且数据无任何丢失，RPO = 0，RTO \< 30s。

   

3. Leader 节点以外的其他少数派节点出现网络分区：由于 Paxos 要求日志只需要同步到 Majority 即可，对可用性无影响，RPO=0，RTO=0。

   

4. 包括 Leader 节点在内的少数派节点出现网络分区：通过无主选举+Paxos 恢复流程，保证在 lease 过期后很短的时间内即选出新的 Leader 提供服务，且数据无任何丢失，RPO = 0，RTO \< 30s。

   

5. 多数派节点宕机：此时服务中断，需要重启宕机进行副本恢复。

   

6. 集群全部宕机：此时服务中断，需要重启宕机进行副本恢复。

   




除上述描述的宕机或网络分区场景外，真实业务的故障场景会更加多样化。OceanBase 数据库除了可以处理上述场景外，对于多种磁盘故障（完全 hang 住或写入缓慢）可以进行自动检测，在 1 分钟内切走 Leader 和备机读流量，保证业务及时恢复。

只读副本、级联同步以及副本类型转换 
--------------------------------------

日志服务通过保证 log_id 和 trans_version 偏序，以及百毫秒级的 Keepalive，用以支持备机和只读副本的弱读功能。

只读副本不是 Paxos Group 的成员，我们通过自适应的结合 Locality 信息的级联算法，自动为只读副本构建上下游关系，实现数据的自动同步。

除只读副本外，日志服务支持日志副本（日志副本有特殊的日志回收策略）以及三种副本类型之间的相互转换。
