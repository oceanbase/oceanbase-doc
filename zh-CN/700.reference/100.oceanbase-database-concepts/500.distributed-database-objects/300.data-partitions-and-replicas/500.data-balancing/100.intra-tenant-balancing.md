|description||
|---|---|
|keywords||
|dir-name||
|dir-name-en||
|tenant-type||

# 租户内均衡

在 OceanBase 数据库 V3.x 版本中，副本以分区为粒度分布在多节点上，负载均衡模块可以通过分区切主、迁移分区副本等方式实现租户内分区均衡。V4.x 版本中升级为单机日志流架构后，副本以日志流为粒度分布，每个日志流上承载着大量分区。因此，在 V4.2.x 版本中，负载均衡模块无法直接控制分区分布，而是需要先对 LS（Log Stream，日志流）的数量和 Leader 进行均衡，然后才能在日志流分布的基础上再进行分区均衡。

<main id="notice" type='notice'>
<h4>注意</h4>
<p>分区均衡仅针对用户表。</p>
</main>

LS 均衡与分区均衡的优先级顺序如下：

`LS 均衡 > 分区均衡`

## LS 均衡

### LS 数量均衡

当用户对租户执行 `UNIT_NUM` 变更、修改 `PRIMARY_ZONE` 的第一优先级、修改 Locality（影响 `PRIMARY_ZONE`）等操作时，负载均衡模块的后台线程会立即通过 LS 分裂、LS 合并、LS Group 变更等动作，变更 LS 的数量和位置，以满足用户修改后的租户状态，即每个租户的 Unit 都有一个日志流组，日志流组内日志流的个数等于 `PRIMARY_ZONE` 的第一优先级的个数，Leader 在日志流组内均衡地分布在各个 Zone 上。

<main id="notice" type='explain'>
<h4>说明</h4>
<p><ul>
<li>LS Group 是 LS 的属性，LS Group ID 相同的日志流会聚合在一起。</li>
<li>从 V4.0.0 版本开始，OceanBase 数据库要求租户内每个 Zone 的 Unit 个数必须保持一致。为了方便统一管理各个 Zone 的 Unit，系统引入了 Unit Group 机制，即不同 Zone 之间相同编号（UNIT_GROUP_ID）的 Unit 属于同一个 Unit Group。Unit Group 内所有 Unit 上的数据分布相同，具有相同的日志流副本，服务相同的分区数据。从资源容器上， Unit Group 框定了一组数据，这组数据由一个或多个日志流服务，读写服务能力可以扩展到 Unit Group 内的多个 Unit 上。</li>
<li>一个 LS Group 唯一对应一个 Unit Group，也就是相同 LS Group ID 的日志流分布于相同的 Unit Group 内。</li>
</ul> </p>
</main>

LS 数量的计算公式如下：

`LS 数量 = UNIT_NUM * first_level_primary_zone_num`

LS 数量均衡的场景如下表所示。

| 触发均衡的场景示例                                       |    均衡条件      |    均衡算法     | LS 均衡策略             |
|--------------------------------------------------------|-----------------|-----------------|------------------------|
| `PRIMARY_ZONE`：从 `z1, z2` 变更为 `z1`</br>同时 `UNIT_NUM` ：从 `1` 变更为 `2` | LS Group 中有的缺少 LS 和有的有多余 LS，但 LS 的总数量符合终态    |   迁移冗余 LS 至缺少 LS 的 LS Group 中             |   LS_BALANCE_BY_MIGRATE  |
| `PRIMARY_ZONE`: 从 `z1` 变更为 `z1,z2` </br>或者 `UNIT_NUM`：从 `2` 变更为 `3`  | 仅存在 LS Group 缺少 LS  | 假设当前 LS 的个数为 M，扩容后 LS 的个数为 N（M < N），则每个缺少的 LS 都需要得到 `M/N` 个 Tablet | LS_BALANCE_BY_EXPAND |
| `PRIMARY_ZONE`: 从 `z1,z2` 变更为 `z1` </br>或者 `UNIT_NUM`：从 `3` 变更为 `2`  | 仅存在 LS Group 有多余 LS | 假设当前 LS 的个数为 M，需要缩容到 N 个 LS 上（M > N），则剩下的每个 LS 都需要分到 `(M-N)/N` 个 Tablet | LS_BALANCE_BY_SHRINK |

示例 1：如下图所示，`PRIMARY_ZONE` 的第一优先级从 `Z1, Z2` 变更为 `Z1`；同时 `UNIT_NUM` 需要从 `1` 变更为 `2`。变更前后，LS 数量不变，直接将 `LS2 `迁移到新建的 Unit 上，`LS2` 根据 `PRIMARY_ZONE` 的变更切主到 `Z1`，最终完成 LS 均衡。

![LS_BALANCE_BY_MIGRATE 示例](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/example1.png)

示例 2：如下图所示，`PRIMARY_ZONE = Z1`，`UNIT_NUM` 从 `1` 变更为 `2`，为保证每个 Unit 都有 LS，分裂出 LS2，带走 1/2 的 Tablet，迁移到新的 Unit 上。

![LS_BALANCE_BY_EXPAND 示例](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/example2.png)

示例 3：如下图所示，`UNIT_NUM = 1`，`PRIMARY_ZONE` 的第一优先级从 `RANDOM` 变更为 `Z1,Z2`。为保证 Leader 仅在 `PRIMARY_ZONE` 第一优先级的 Zone 上，还需要保证均衡完后，日志流上的 Tablet 的数量均衡。由于 `LS3` 与其余 LS 在同一个 LS Group 中，可以直接从 `LS3` 上 Transfer 走 1/2 的 Tablet 到 `LS1` 上，然后再将 `LS3` 合并到 `LS2` 上，让 `LS2` 承载剩余 1/2 的 Tablet，就可以在减少 LS 数量的同时保持 Tablet 数量均衡。

![LS_BALANCE_BY_SHRINK 示例](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/example3.png)

### LS Leader 均衡

LS Leader 均衡是在 LS 数量均衡的前提下，让 LS Leader 平均分布在 Primary Zone 上。例如，用户变更租户的 `PRIMARY_ZONE` 后，负载均衡后台线程会根据 `PRIMARY_ZONE` 的第一优先级动态调整 LS 数量，之后再对 LS 进行切主，保证第一优先级的各 Zone 内均有且仅有一个 LS Leader。

例 1：如下图所示，`UNIT_NUM = 1`，`PRIMARY_ZONE` 的第一优先级从 `Z1` 变更为 `Z1,Z2`，为保证 `PRIMARY_ZONE` 第一优先级的所有 Zone 上都有 Leader，在同一个 Unit 里分裂出新的 LS，然后再通过 LS Leader 均衡将新 LS 的 Leader 切到 `Z2`。

![Primary Zone 变更示例](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/change_primary_zone_example.png)

例 2：如下图所示， 当 `UNIT_NUM = 1`，`PRIMARY_ZONE` 的第一优先级从 `RANDOM` 变更为 `Z1,Z2`时，经过 LS 数量均衡将 `LS3` 合并入其他 LS 后，剩余的 LS Leader 天然平均分布在`Z1` 和 `Z2` 上，无需额外的 LS Leader 均衡。

![LS_BALANCE_BY_SHRINK 示例](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/example3.png)

## 建表时分区的分配

当用户创建用户表时，OceanBase 数据库会采用一套均衡分配策略将分区打散或聚合到各个用户日志流上，以保证各个日志流上分区的相对均衡。

### 指定 Table Group 的用户表

用户可以通过指定 Table Group，灵活配置不同表之间的聚集、打散规则。在创建用户表时，如果指定了对应 Sharding 属性的 Table Group，则新建用户表的分区会按照 Table Group 的规则分配到对应用户的 LS 上。Table Group 的规则如下表所示。

| Table Group 的 Sharding 属性 | 说明                   | 分区方式要求                       | 对齐规则                                              |
|-----------------------------|------------------------|-----------------------------------|-------------------------------------------------------|
| NONE      | 所有表的所有分区都聚集在同一台 Server       | 无限制                            | 所有分区与表组中 `table_id` 最小的表的分区聚集在同一个 LS |
| PARTITION | 按一级分区打散。如果是二级分区表，则一级分区下的所有二级分区聚集在一起 | 要求所有表的一级分区的分区方式相同；如果是二级分区表，也只校验一级分区的分区方式。因此，一级分区表和二级分区表可以同时存在，只要他们的一级分区的分区方式相同即可。</p>| 相同一级分区 Value 的分区聚集在一起，包括：一级分区表的一级分区和二级分区表的对应一级分区下的所有二级分区。|
| ADAPTIVE  | 自适应打散方式。如果 Table Group 内是一级分区表，则按一级分区打散；如果Table Group 内是二级分区表，则按每个一级分区下的二级分区打散。| <ul><li>全部是一级分区表，或者全部是二级分区表。</li><li>如果是一级分区表，则要求一级分区的分区方式相同；如果是二级分区表，则要求一级和二级分区方式都相同。</li></ul>| <ul><li>一级分区表：一级分区 Value 相同的分区聚集在一起。 </li><li>二级分区表：一级分区 Value 相同，并且二级分区 Value 相同的分区聚集在一起。</li></ul> |

<main id="notice" type='notice'>
<h4>注意</h4>
<p>如果在建表前，Table Group 中已存在用户表，后续新建的表会按 Table Group 中 <code>table_id</code> 最小的用户表对应的分区分布对齐。</p>
</main>

有关 Table Group 的更多介绍，请参见 [关于表组](../../../../300.database-object-management/100.manage-object-of-mysql-mode/400.manage-table-groups-of-mysql-mode/100.about-table-groups-of-mysql-mode.md)。

### 普通用户表

不指定 Table Group 创建普通用户表时，OceanBase 数据库默认将新建分区打散到所有用户 LS 上。具体规则如下：

* 非分区表：新建分区分配到分区数最少的 LS 上。

* 一级分区表：一级分区按序切分平均分布到所有的用户 LS 上。

* 二级分区表：每个一级分区下所有二级分区按序切分平均分布所有的用户 LS 上。

下面通过几个示例来进行说明。

* 示例 1：在 MySQL 模式下分别创建了 `tt1`、`tt2`、`tt3`、`tt4` 等 4 张非分区表。

    ```sql
    obclient [test]> CREATE TABLE tt1(c1 int);
    ```

    ```sql
    obclient [test]> CREATE TABLE tt2(c1 int);
    ```

    ```sql
    obclient [test]> CREATE TABLE tt3(c1 int);
    ```

    ```sql
    obclient [test]> CREATE TABLE tt4(c1 int);
    ```

    查询 4 张表的分区分布情况。

    ```sql
    obclient [test]> SELECT table_name,partition_name,subpartition_name,ls_id,zone FROM oceanbase.DBA_OB_TABLE_LOCATIONS WHERE table_name in('tt1','tt2','tt3','tt4') AND role='LEADER';
    ```

    查询结果如下。

    ```shell
    +------------+----------------+-------------------+-------+------+
    | table_name | partition_name | subpartition_name | ls_id | zone |
    +------------+----------------+-------------------+-------+------+
    | tt1        | NULL           | NULL              |  1001 | z1   |
    | tt2        | NULL           | NULL              |  1002 | z2   |
    | tt3        | NULL           | NULL              |  1003 | z3   |
    | tt4        | NULL           | NULL              |  1001 | z1   |
    +------------+----------------+-------------------+-------+------+
    4 rows in set
    ```

* 示例 2：再创建一个一级分区表 `tt5`。

    ```sql
    obclient [test]> CREATE TABLE tt5(c1 int) PARTITION BY HASH(c1) PARTITIONS 6;
    ```

    查看 `tt5` 分区表的分区分布情况。

    ```sql
    obclient [test]> SELECT table_name,partition_name,subpartition_name,ls_id,zone FROM oceanbase.DBA_OB_TABLE_LOCATIONS WHERE table_name ='tt5' AND role='LEADER';
    ```

    查询结果如下。该表的一级分区按顺序平均分布到了 `1001`、`1002`、`1003` 等日志流上。

    ```shell
    +------------+----------------+-------------------+-------+------+
    | table_name | partition_name | subpartition_name | ls_id | zone |
    +------------+----------------+-------------------+-------+------+
    | tt5        | p0             | NULL              |  1002 | z2   |
    | tt5        | p1             | NULL              |  1002 | z2   |
    | tt5        | p2             | NULL              |  1003 | z3   |
    | tt5        | p3             | NULL              |  1003 | z3   |
    | tt5        | p4             | NULL              |  1001 | z1   |
    | tt5        | p5             | NULL              |  1001 | z1   |
    +------------+----------------+-------------------+-------+------+
    6 rows in set
    ```

* 示例 3：创建一个二级分区表 `tt8`。

    ```sql
    obclient [test]> CREATE TABLE tt8 (c1 int, c2 int, PRIMARY KEY(c1, c2)) 
        PARTITION BY HASH(c1) SUBPARTITION BY RANGE(c2) SUBPARTITION TEMPLATE 
        (SUBPARTITION p0 VALUES LESS THAN (1990), 
         SUBPARTITION p1 VALUES LESS THAN (2000),
         SUBPARTITION p2 VALUES LESS THAN (3000),
         SUBPARTITION p3 VALUES LESS THAN (4000),
         SUBPARTITION p4 VALUES LESS THAN (5000),
         SUBPARTITION p5 VALUES LESS THAN (MAXVALUE)) 
         PARTITIONS 2;
    ```

    查看表 `tt8` 的分区分布情况。

    ```sql
    obclient [test]> SELECT table_name,partition_name,subpartition_name,ls_id,zone FROM oceanbase.DBA_OB_TABLE_LOCATIONS WHERE table_name ='tt8' AND role='LEADER';
    ```

    查询结果如下：

    ```shell
    +------------+----------------+-------------------+-------+------+
    | table_name | partition_name | subpartition_name | ls_id | zone |
    +------------+----------------+-------------------+-------+------+
    | tt8        | p0             | p0sp0             |  1003 | z3   |
    | tt8        | p0             | p0sp1             |  1003 | z3   |
    | tt8        | p0             | p0sp2             |  1001 | z1   |
    | tt8        | p0             | p0sp3             |  1001 | z1   |
    | tt8        | p0             | p0sp4             |  1002 | z2   |
    | tt8        | p0             | p0sp5             |  1002 | z2   |
    | tt8        | p1             | p1sp0             |  1001 | z1   |
    | tt8        | p1             | p1sp1             |  1001 | z1   |
    | tt8        | p1             | p1sp2             |  1002 | z2   |
    | tt8        | p1             | p1sp3             |  1002 | z2   |
    | tt8        | p1             | p1sp4             |  1003 | z3   |
    | tt8        | p1             | p1sp5             |  1003 | z3   |
    +------------+----------------+-------------------+-------+------+
    12 rows in set
    ```

### 特殊用户表

除了普通用户表外，有局部索引表、全局索引表、复制表这些特殊用户表，具有特别的分配规则。具体如下：

* 局部索引表：与用户表主表的分区规则相同，每个分区与主表的分区分布绑定在一起。

* 全局索引表：默认为非分区表，建表时分布到分区数量最少的 LS 上。

* 复制表：复制表仅存在于广播日志流上。

## 分区均衡

在 LS 均衡的基础上，负载均衡模块会通过 Transfer，将分区 Tablet 打散或聚合在不同的 LS 上，达到租户下的分区均衡。分区均衡时间间隔通过租户级配置项 `partition_balance_schedule_interval` 控制，默认 2h 执行一次，每次执行仅运行一种分区均衡策略。如果配置项 `partition_balance_schedule_interval` 的值设置为 `0s`，则表示不进行分区均衡。

分区均衡策略的优先级顺序如下：

`分区属性对齐 > 分区数量均衡 > 分区磁盘均衡`

### 分区属性对齐

**Table Group 对齐**

当用户通过 SQL 语句修改表的 Table Group 属性或 Table Group 的 Sharding 属性时，需要等待后台的分区均衡任务执行完成，才能达到符合预期的分区分布。可以临时调小租户级配置项 `partition_balance_schedule_interval` 的值以便快速对齐。

Table Group 中表的分区分布规则请参见本文 **指定 Table Group 的用户表** 中的内容。

有关 Table Group 的更多介绍，请参见 [关于表组](../../../../300.database-object-management/100.manage-object-of-mysql-mode/400.manage-table-groups-of-mysql-mode/100.about-table-groups-of-mysql-mode.md)。

### 分区数量均衡

分区数量均衡的目标是让所有用户的 LS 上的用户表主表分区数量均匀（数量偏差不大于 1）。

实现上，OceanBase 数据库采用 “均衡组” 的概念描述打散关系，将需要打散分布的分区划分到同一个均衡组内，通过组内均衡和组间均衡的方式实现分区数量均衡。

无 Table Group 的场景下，均衡组的划分方式如下：

| 表类型    | 均衡组划分                                 | 分布方式（无 Table Group）                              |
|-----------|-------------------------------------------|---------------------------------------------------------|
| 非分区表   | 租户下所有非分区表是一个均衡组。             | 租户下所有非分区表，平均分布到用户的所有 LS 上（数量偏差不大于 1）。|
| 一级分区表 | 每张表下所有一级分区是一个均衡组。           | 以每张表为单位，一级分区平均分布到用户的所有 LS 上。|
| 二级分区表 | 单张表的每个一级分区下的所有二级分区是一个均衡组。| 单张表每个一级分区下的所有二级分区，平均分布到用户的所有 LS 上。|

均衡阶段，系统先对所有均衡组进行组内均衡，即组内分区平均分布，然后再在组内均衡的基础上，从分区数最多的 LS 上 Transfer 走一部分分区，进行组间均衡，从而实现分区数量均衡。

例如，现有 4 张表的所有分区均在 `LS1` 上，分为 3 个均衡组：

* 均衡组 1：非分区表 `non_part_t1、non_part_t2`

* 均衡组 2：一级分区表的 2 个分区 `part_one_t3_p0`、`part_one_t3_p1`

* 均衡组 3：二级分区表的 4 个分区 `part_two_t4_p0s0`、`part_two_t4_p0s1`、`part_two_t4_p1s0`、`part_two_t4_p1s1`

初始分布，各分区分布为 8-0-0。

![初始分布](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/initial-distribution.png)

组内均衡后，各分区分布为 4-3-1，各均衡组已达到均衡，但总体未均衡。

![组内均衡后分布](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/distribution2.png)

组间均衡后，分区分布为 3-3-2，从分区最多的 `LS1` 上 Transfer 走一个分区到分区数最少的 `LS3` 上，总体达到均衡。

![组间均衡后分布](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/distribution3.png)

当存在 Table Group 时，每个 Table Group 是一个独立均衡组，系统会将 Table Group 要求聚集的分区进行绑定，视为一个分区。然后采用同样的组内均衡、组间均衡方式进行分区数量均衡。存在 Table Group 时，租户下所有分区数量可能无法达到完全平均。

存在 Table Group 的场景下，均衡组的划分方式如下：

| 表类型                      | 均衡组划分                                 | 分布方式                                                 |
|----------------------------|-------------------------------------------|---------------------------------------------------------|
| Table Group 的 Sharding 属性为 `NONE` 的表 | 自成一个均衡组              | Table Group 下所有分区都分布在一个日志流上                 |
| Table Group 的 Sharding 属性为 `PARTITION` 或 `ADAPTIVE` 的一级分区表 | 所有表的一级分区是一个均衡组 | 第一张表平均分区到所有的日志流上，后面的表的位置均与第一张表聚集 |
| Table Group 的 Sharding 属性为 `ADAPTIVE` 的二级分区表| 所有表一级分区下的所有二级分区是一个均衡组 | 第一张表每个一级分区下的所有二级分区平均打散在所有日志流上，后面的表的位置均与第一张票聚集 |

例如，在上面均衡后的基础上，再新增一个 `sharding = 'NONE'` 的 Table Group `tg1`，其中包含 `non_part_t5_in_tg1`、`non_part_t6_in_tg1`、`non_part_t7_in_tg1`、`non_part_t8_in_tg1` 等四个非分区表。由于 `tg1` 中的所有表必须绑定到一起，均衡后表的分区分布为 4-3-5。

![含 Table Group 的分布](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.2/system-principle/distribution-with-table-group.png)

### 分区磁盘均衡

在分区数量均衡的基础上，负载均衡模块通过尽可能地交换分区，实现各个日志流之间的磁盘占用差值不超过集群级配置项 `server_balance_disk_tolerance_percent` 所设置的百分比。如果单分区磁盘占用过大，可能会达不到该均衡效果。

为避免小数据量场景均衡过于频繁的问题，当前版本分区磁盘均衡触发的阈值为 "50GB"，若单个 LS 磁盘占用小于该阈值，则不会触发磁盘均衡。

### 分区均衡的判断方法

分区均衡是用户表主表的均衡，查询时需要指定 `table_type='USER TABLE'`。分区磁盘均衡主要通过视图 `CDB_OB_TABLET_REPLICAS` 中的 `data_size` 字段来判断。

* 分区数量均衡的查询方法

  ```sql
  obclient [test]> SELECT svr_ip,svr_port,ls_id,count(*) FROM oceanbase.CDB_OB_TABLE_LOCATIONS WHERE tenant_id=xxx AND role='leader' AND table_type='USER TABLE' GROUP BY svr_ip,svr_port,ls_id;
  ```

* 分区磁盘均衡的查询方法

  ```sql
  obclient [test]> SELECT a.svr_ip,a.svr_port,b.ls_id,sum(data_size)/1024/1024/1024 as total_data_size FROM oceanbase.CDB_OB_TABLET_REPLICAS a, oceanbase.CDB_OB_TABLE_LOCATIONS b WHERE a.tenant_id=b.tenant_id AND a.svr_ip=b.svr_ip AND a.svr_port=b.svr_port AND a.tablet_id=b.tablet_id AND b.role='leader' AND b.table_type='USER TABLE' AND a.tenant_id=xxxx GROUP BY svr_ip,svr_port,ls_id;
  ```
