# 在 Kubernetes 集群中部署 OceanBase 数据库

本文介绍如何通过 ob-operator 在 Kubernetes 集群中部署 OceanBase 数据库, 目前我们测试了 K8s 1.23.6 版本，其他版本中部署如果有问题，欢迎和我们联系。
本文主要分为三部分
1. 部署 ob-operator
2. 部署 OceanBase
2. 监控 OceanBase

## 部署 ob-operator
ob-operator 是一个 K8s operator, 可以简化 OceanBase 在 K8s 中的部署和运维。
ob-operator 可以通过两种方式进行部署

### 使用 Helm 部署 ob-operator

```
helm repo add ob-operator https://oceanbase.github.io/ob-operator/
helm install ob-operator ob-operator/ob-operator --namespace=oceanbase-system --create-namespace  --version=1.1.0
```

### 使用配置文件部署 ob-operator

```
# deploy CRD
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/crd.yaml

# deploy ob-operator
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/operator.yaml
```

### ob-operator 自定义
如果您需要对 ob-operator 进行自定义修改，可以通过下载配置文件，修改后执行如下命令进行部署
```bash
# download the config file
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/crd.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/operator.yaml

# after making some modification
kubectl apply -f crd.yaml
kubectl apply -f operator.yaml
```

## 部署 OceanBase

### 部署前准备

#### 配置 Kubernetes 节点 label

ob-operator 会根据 OceanBase 的配置文件 `obcluster.yaml` 中的 `nodeSelector` 来决定 observer 节点的分布，因此需要首先对 K8s 的 node 节点配置 label。

您可以通过如下命令来设置 node 的 label

```bash
kubectl label node ${node_name} ${label_key}=${label_value}

# for example
kubectl label node node1 ob.zone=zone1
```

#### 部署 local-path-provisioner

ob-operator 在部署 OceanBase 时需要创建 PVC 作为 OceanBase 的存储，出于性能考虑，推荐使用本地存储, 本文中使用 local-path-provisioner 来管理 PVC。

部署命令:
``` bash
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.24/deploy/local-path-storage.yaml
```

更多信息请参考 <https://github.com/rancher/local-path-provisioner>

### 部署 OceanBase 集群

OceanBase 集群通过 yaml 配置文件进行定义，您可参考 ob-operator 提供的[配置文件](https://github.com/oceanbase/ob-operator/blob/master/deploy/obcluster.yaml)进行自定义的修改。
您可通过如下命令下载 obcluster 的配置文件。
```bash
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obcluster.yaml
```

配置文件内容示例如下

```yaml
apiVersion: cloud.oceanbase.com/v1
kind: OBCluster
metadata:
  name: ob-test
  namespace: obcluster
spec:
  imageRepo: oceanbasedev/oceanbase-cn
  tag: v4.1.0.0-100000192023032010
  imageObagent: oceanbase/obagent:1.2.0
  clusterID: 1
  topology:
    - cluster: cn
      zone:
      - name: zone1
        region: region1
        nodeSelector:
          ob.zone: zone1
        replicas: 1
      - name: zone2
        region: region1
        nodeSelector:
          ob.zone: zone2
        replicas: 1
      - name: zone3
        region: region1
        nodeSelector:
          ob.zone: zone3
        replicas: 1
      parameters:
        - name: log_disk_size
          value: "40G"
  resources:
    cpu: 2
    memory: 10Gi
    storage:
      - name: data-file
        storageClassName: "local-path"
        size: 50Gi
      - name: data-log
        storageClassName: "local-path"
        size: 50Gi
      - name: log
        storageClassName: "local-path"
        size: 30Gi
      - name: obagent-conf-file
        storageClassName: "local-path"
        size: 1Gi
    volume:
        name: backup
        nfs:
          server: ${nfs_server_address}
          path: /opt/nfs
          readOnly: false
```

主要配置介绍：

- `imageRepo`：OceanBase 镜像 repo。
- `tag`：OceanBase 镜像 tag。
- `imageObagent`: OBAgent 的镜像，用来做 OceanBase 的监控数据采集。
- `cluster`：按需配置，如果需要在该 Kubernetes 集群中部署 OceanBase 集群，请将 cluster 配置为与 ob-operator 配置文件中的启动参数 `--cluster-name` 相同 (默认值为cn) 。
- `nodeSelector`: 当前 zone 的 node 选择策略，按照 node 的 label 进行匹配, 该配置即上文部署前准备中配置 Kubernetes 节点 label 所用配置。
- `parameters`：按需配置, 自定义的 OceanBase 配置项。
- `cpu`：Pod 的 CPU 资源，要求配置值 >= 2。
- `memory`：Pod 的 Memory 资源, 要求配置为大于 10 Gi 的整数，小于 10 Gi 会引发系统异常。
- `data-file`：OceanBase 数据存储配置，可以指定大小和 `storageClass`, 用作 OceanBase 的数据目录， 建议为大小至少配置为 memory 大小的 3 倍以上。
- `data-log`：OceanBase 日志存储配置, 可以指定大小和 `storageClass`, 用作 OceanBase 的 Clog 目录， 建议大小至少配置为 memory 大小的 5 倍以上。
- `log`：OceanBase 进程日志存储配置，可以指定大小和 `storageClass`, 用作 OceanBase 进程日志的存放目录, 建议大小至少配置 30Gi 以上。
- `obagent-conf-file`：OBAgent 配置文件目录存储配置, 用来存放 OBAgent 配置文件, 不需要配置特别打，一般配置 1Gi 就可以。
- `volume`: 用作备份数据的存储，无备份需求可以不配置，但是集群创建好之后无法再另外添加，需要提前做好规划。

配置文件修改之后可以通过以下命令进行部署:
```
kubectl apply -f obcluster.yaml
```


### 部署 OBProxy
OBProxy 通过 yaml 配置文件进行定义，您可参考 ob-operator 提供的[配置文件](https://github.com/oceanbase/ob-operator/tree/master/deploy/obproxy)进行自定义的修改。
您可通过如下命令下载 OBProxy 的配置文件。
```bash
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/deployment.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/service.yaml
```
参考以下配置文件进行修改：
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: obproxy
  namespace: obcluster
spec:
  selector:
    matchLabels:
      app: obproxy
  replicas: 2
  template:
    metadata:
      labels:
        app: obproxy
    spec:
      containers:
        - name: obproxy
          image: oceanbasedev/obproxy-ce:4.1.0.0-7
          ports:
            - containerPort: 2883
              name: "sql"
            - containerPort: 2884
              name: "prometheus"
          env:
            - name: APP_NAME
              value: helloworld
            - name: OB_CLUSTER
              value: ob-test
            - name: RS_LIST
              value: $(SVC_OB_TEST_SERVICE_HOST):$(SVC_OB_TEST_SERVICE_PORT)
          resources:
            limits:
              memory: 2Gi
              cpu: "1"
```
```yaml
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: obproxy-service
  namespace: obcluster
spec:
  type: NodePort
  selector:
    app: obproxy
  ports:
    - name: "sql"
      port: 2883
      targetPort: 2883
      nodePort: 30083
    - name: "prometheus"
      port: 2884
      targetPort: 2884
      nodePort: 30084
```

主要环境变量和配置说明：

- `APP_NAME`: OBProxy 的 appname。
- `OB_CLUSTER`: OBProxy 连接的 OceanBase 集群名。
- `RS_LIST`: OceanBase 集群的 rs_list, 这里是通过两个环境变量 `SVC_OB_TEST_SERVICE_HOST`, `SVC_OB_TEST_SERVICE_PORT` 来配置，环境变量的名字和 OceanBase 的集群名有关, 其中 `OB_TEST` 部分是 OceanBase 集群名, 需要转换成大写和下划线。
- `service`: service 配置中会开放两个端口，一个用来作 sql 连接的端口，一个是监控数据的采集端口。

配置文件修改之后可以通过以下命令进行部署:
```
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
```

### 连接 OceanBase 集群
推荐通过 OBProxy 来连接 OceanBase，在部署好了 OceanBase 和 OBProxy 之后，您可通过以下命令来获取 OBProxy 的 service 连接地址。
```bash
kubectl get svc ${servicename} -n ${namespace}

# for example
kubectl get svc obproxy-service -n obcluster

# output
NAME              TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
obproxy-service   NodePort   1.1.1.1   <none>        2883:30083/TCP,2884:30084/TCP   1m
```

可以通过 CLUSTER-IP 和 PORT 的方式进行连接，对应的连接命令为
```bash
# without password
mysql -h1.1.1.1 -P2883 -uroot oceanbase -A -c

# with password
mysql -h1.1.1.1 -P2883 -uroot -p oceanbase -A -c
```


## 监控 OceanBase

### 部署 prometheus

#### 部署方式
您可根据 ob-operator 中的 prometheus [配置文件](https://github.com/oceanbase/ob-operator/tree/master/deploy/prometheus)作为参考，可以通过如下命令下载配置文件
```bash
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role-binding.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/configmap.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/deployment.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/service.yaml
```

prometheus 根据 K8s 中服务的名称和端口来选择收集数据的地址，可以按照正则表达式进行过滤, 可以在 config.yaml 中进行配置

```yaml
# configmap.yaml中部分配置
scrape_configs:
  - job_name: 'obagent-monitor-basic'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/basic'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-extra'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/extra'
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_pod_container_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-host'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/node/host'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'proxy-monitor'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: obproxy-service;prometheus
      action: keep
```

#### 配置说明：

- service 和 port 说明:

  - OceanBase: ob-operator 会根据部署的 OceanBase 集群名称来创建一个 service 用来获取 exporter 地址， service 命名的规则是 `svc-monitor-${obcluster_name}`, 端口名为 `monagent`, 如果部署 OceanBase 的时候自定义了集群名，需要根据实际的集群名称来修改对应配置
  - OBProxy: 需要配置对应的 OBProxy 的 service 和 port name

- 监控指标的请求路径:

  - OceanBase 监控指标： 请求路径分别为 `/metrics/ob/basic`, `/metrics/ob/extra`
  - OBProxy 监控指标: OBProxy 自身支持了 prometheus 协议暴露监控指标的能力，请求路径为 `/metrics`

配置文件修改之后可以通过以下命令进行部署:
```
kubectl apply -f cluster-role.yaml
kubectl apply -f cluster-role-binding.yaml
kubectl apply -f configmap.yaml
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
```

#### 验证

浏览器打开 prometheus 的地址，查看 Status -> Targets, 验证 `proxy-monitor`, `obagent-monitor-host`, `obagent-monitor-basic`, `obagent-monitor-extra` 下的 Endpoint 都是 up 状态
点击 Graph, 可以输入 `PromQL` 表达式进行查询

### 部署 grafana

#### 部署方式
您可根据 ob-operator 中的 grafana [配置文件](https://github.com/oceanbase/ob-operator/tree/master/deploy/grafana)作为参考，可以通过如下命令下载配置文件
```bash
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/configmap.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/pvc.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/deployment.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/service.yaml
```

grafana 通过 configmap 设置配置信息

```yaml
# configmap 中关键配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: obcluster
data:
  prometheus.yaml: |-
    {
        "apiVersion": 1,
        "datasources": [
            {
               "access":"proxy",
                "editable": true,
                "name": "prometheus",
                "orgId": 1,
                "type": "prometheus",
                "url": "http://svc-prometheus.obcluster.svc:8080",
                "version": 1,
                "isDefault": true
            }
        ]
    }
```

配置说明：

- `url`：grafana 的配置中会使用到 prometheus 的服务地址作为数据源的配置(datasource 中的 url 字段), 配置的格式为 "http://${service_name}.${namespace}.svc:${service_port}"

配置文件修改之后可以通过以下命令进行部署:
```
kubectl apply -f configmap.yaml
kubectl apply -f pvc.yaml
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
```

#### 验证

浏览器打开 grafana 的地址，通过 `admin` 用户登陆，默认密码也是 `admin`, 第一次会提示修改密码
grafana 的配置中默认携带了 OceanBase 的 dashboard，可以在浏览器中打开以下链接查看
```text
http://${node_ip}:${node_port}/d/oceanbase
```

主机指标和 obproxy 的监控模版可以通过添加如下 id 对应的 dashboard 进行查看
- 主机: 15216
- obproxy: 15354

  <main id="notice" type='explain'>
    <h4>说明</h4>
    <p>obproxy 需要有实际请求之后，才会有监控数据展示</p>
  </main>
