# 在 Kubernetes 集群中部署 OceanBase 数据库

本文介绍如何通过 ob-operator 在 Kubernetes 集群中部署 OceanBase 数据库。

## 背景信息

ob-operator 与其他 operator 一样，旨在让 OceanBase 以容器的方式，无缝运行在 Kubernetes 集群上。
ob-operator 现已支持 OceanBase 集群的创建、删除，完整的节点生命周期管理、ob-proxy 管理，后续会支持租户管理、多 Kubernetes 集群等功能。
您可按照以下步骤通过 ob-operator 来部署 OceanBase 数据库。

## 部署 CRD

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/crd.yaml
```

## 部署 ob-operator

### 使用默认配置部署 ob-operator

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/operator.yaml
```

### 自定义部署 ob-operator

您也可以根据您的配置修改 `operator.yaml`。
例如在 operator.yaml 中添加启动参数 `--cluster-name`，含义为 ob-operator 只处理 `cluster` 值与自身启动参数 `--cluster-name` 的值相同的 CRD。
>**注意**
>
>`--cluster-name` 的值需要与 obcluster 配置中的 `cluster` 配置一致。
参考以下部分配置文件进行修改：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    control-plane: controller-manager
  name: ob-operator-controller-manager
  namespace: oceanbase-system
spec:
  replicas: 1
  selector:
    matchLabels:
      control-plane: controller-manager
  template:
    metadata:
      labels:
        control-plane: controller-manager
    spec:
      containers:
      - args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=127.0.0.1:8080
        - --leader-elect
        - --cluster-name=cn
        command:
        - /manager
        image: ob-operator:latest
        imagePullPolicy: Always
        name: manager
```

## 配置 Kubernetes 节点 label

您需将 Kubernetes 节点配置 label，label 需要与 obcluster.yaml 中的 `nodeSelector` 配置相匹配。ob-operator 会将 Pod 调度到具有相应 label 的节点上。
推荐配置 label 的 key 为 `topology.kubernetes.io/zone`。不同 Zone 推荐配置不同的 label 以做容灾。

```bash
kubectl label node nodename topology.kubernetes.io/zone=zonename
kubectl label node ${node_name} topology.kubernetes.io/zone=${zone_name}
```

## 部署 local-path-provisioner

ob-operator 默认配置中使用了 local-path-provisioner，需要提前准备好

部署命令:

```text
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
```

更多信息可以参考 <https://github.com/rancher/local-path-provisioner>

## 部署 OceanBase 集群

### 使用默认配置部署 OceanBase 集群

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obcluster.yaml
```

### 自定义部署 OceanBase 集群

您也可以在 obcluster.yaml 中根据自己的配置做一些修改。
参考以下部分配置文件进行修改：

```yaml
apiVersion: cloud.oceanbase.com/v1
kind: OBCluster
metadata:
  name: ob-test
  namespace: obcluster
spec:
  imageRepo: oceanbasedev/oceanbase-cn
  tag: 4.0.0-snapshot-20230130110559
  clusterID: 1
  topology:
    - cluster: cn
      zone:
      - name: zone1
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone1
        replicas: 1
      - name: zone2
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone2
        replicas: 1
      - name: zone3
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone3
        replicas: 1
      parameters:
        - name: freeze_trigger_percentage
          value: "30"
  resources:
    cpu: 2
    memory: 10Gi
    storage:
      - name: data-file
        storageClassName: "local-path"
        size: 50Gi
      - name: data-log
        storageClassName: "local-path"
        size: 50Gi
      - name: log
        storageClassName: "local-path"
        size: 30Gi
```

主要配置介绍：

- `imageRepo`：OceanBase 镜像 repo。
- `tag`：OceanBase 镜像 tag。

- `cluster`：按需配置，如果需要在该 Kubernetes 集群中部署 OceanBase 集群，请将 cluster 配置为与 ob-operator 启动参数 `--cluster-name` 相同。
- `parameters`：按需配置, 自定义的一些 OceanBase 配置项。

- `cpu`：建议配置为大于 2 的整数，小于 2 会引发系统异常。

- `memory`：建议配置为大于 10 Gi 的整数，小于 10 Gi 会引发系统异常。

- `data-file`：该部分为 OBServer 系统配置项 `datafile_size` 的大小，建议为 memory 的 3 倍以上。用户可以自行按需配置 `storageClassName`。

- `data-log`：该部分为 OBServer 系统配置项 `data_dir` 的大小，建议为 memory 的 5 倍以上。用户可以自行按需配置 `storageClassName`。

- `log`：该部分为 OBServer 系统日志的大小，建议为 30 Gi 以上。用户可以自行按需配置 `storageClassName`。

## 部署 OBProxy

### 使用默认配置部署 OBProxy

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/service.yaml
```

### 自定义部署 OBProxy

您也可以在 obproxy.yaml 中根据自己的配置做一些修改。
参考以下部分配置文件进行修改：

```yaml
# deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: obproxy
  namespace: obcluster
spec:
  selector:
    matchLabels:
      app: obproxy
  replicas: 3
  template:
    metadata:
      labels:
        app: obproxy
    spec:
      containers:
        - name: obproxy
          image: oceanbase/obproxy-ce:3.2.3
          ports:
            - containerPort: 2883
              name: "sql"
            - containerPort: 2884
              name: "prometheus"
          env:
            - name: APP_NAME
              value: helloworld
            - name: OB_CLUSTER
              value: ob-test
            - name: RS_LIST
              value: $(SVC_OB_TEST_SERVICE_HOST):$(SVC_OB_TEST_SERVICE_PORT)
          resources:
            limits:
              memory: 2Gi
              cpu: "1"
---
# service
apiVersion: v1
kind: Service
metadata:
  name: obproxy-service
  namespace: obcluster
spec:
  type: NodePort
  selector:
    app: obproxy
  ports:
    - name: "tcp"
      port: 2883
      targetPort: 2883
      nodePort: 30083
```

主要环境变量配置说明：

- `APP_NAME`: OBProxy 的 appname。
- `OB_CLUSTER`: OBProxy 连接的 OceanBase 集群名。

- `RS_LIST`: OceanBase 集群的 rs_list, 这里是通过 OceanBase 集群的 service 地址来生成的，当部署好 OceanBase 集群之后，会自动产生两个环境变量 `SVC_OB_TEST_SERVICE_HOST`, `SVC_OB_TEST_SERVICE_PORT`, 分别表示 OceanBase 集群的服务地址和端口。

## 部署 prometheus

### 使用默认配置部署

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role-binding.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/service.yaml
```

### 自定义配置

prometheus 查询 obagent 的请求地址通过 configmap 来进行配置，根据服务名，端口名来进行过滤, 如果 oceanbase 集群或者 obproxy 集群进行过自定义的配置，需要根据实际情况设置服务名的正则表达式。

```yaml
# configmap 中关键配置
scrape_configs:
  - job_name: 'obagent-monitor-basic'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/basic'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-extra'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/extra'
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_pod_container_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-host'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/node/host'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'proxy-monitor'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: obproxy-service;prometheus
      action: keep
```

利用 k8s 的服务发现能力，配置服务名称和端口名，可以自动找到所有的 endpoint 节点, 根据对应的请求路径，可以查询到对应的监控数据。

配置说明：

- service 和 port 说明:

  - oceanbase: ob-operator 会根据部署的 oceanbase 集群名称来创建一个 service， 命名的规则是 `svc-monitor-${obcluster_name}`, 端口名为 `monagent`, 如果部署 oceanbase 的时候自定义了集群名，需要根据实际的集群名称来修改对应配置
  - obproxy: 需要配置对应的 obproxy 的 service 和 port name

- 监控指标的请求路径:

  - oceanbase 监控指标： 请求路径分别为 `/metrics/ob/basic`, `/metrics/ob/extra`

  - obproxy 监控指标: obproxy 自身支持了 prometheus 协议暴露监控指标的能力，请求路径为 `/metrics`

### 验证

浏览器打开 prometheus 的地址，查看 Status -> Targets, 验证 `proxy-monitor`, `obagent-monitor-host`, `obagent-monitor-basic`, `obagent-monitor-extra` 下的 Endpoint 都是 up 状态
点击 Graph, 可以输入 `PromQL` 表达式进行查询

## 部署 grafana

### 使用默认配置部署

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/pvc.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/service.yaml
```

### 自定义配置

grafana 通过 configmap 设置关键配置信息，包括 prometheus 数据源的地址, 如果 prometheus 进行了自定义部署，需要配置实际的 service 地址

```yaml
# configmap 中关键配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: obcluster
data:
  prometheus.yaml: |-
    {
        "apiVersion": 1,
        "datasources": [
            {
               "access":"proxy",
                "editable": true,
                "name": "prometheus",
                "orgId": 1,
                "type": "prometheus",
                "url": "http://svc-prometheus.obcluster.svc:8080",
                "version": 1,
                "isDefault": true
            }
        ]
    }
```

配置说明：

- `url`：grafana 的配置中会使用到 prometheus 的服务地址作为数据源的配置(datasource 中的 url 字段)，需要根据 prometheus 的实际部署情况来填写

### 验证

浏览器打开 grafana 的地址，通过 `admin` 用户登陆，默认密码也是 `admin`, 第一次会提示修改密码
有三个配置好的 dashboard 可以直接导入

- oceanbase: 15215
- 主机: 15216
- obproxy: 15354
导入之后，可以查看对应的监控图表

>**说明**
>
> obproxy 需要有实际请求之后，才会有监控数据展示
