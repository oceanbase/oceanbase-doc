# 导入数据概述

数据流动是面向分析的业务场景中最关键的环节之一。本文介绍如何将外部数据迁移到 OceanBase 数据库中。针对不同的数据来源，本文提供以下数据导入方案。

* 数据库

  例如 MySQL、Oracle、SQL Server、PostgreSQL 和 MongoDB 等。

  应用场景：无缝对接，保持数据结构与完整性。

* 离线数据文件

  CSV、JSON 和 XML 等。

  应用场景：适用于批量数据处理，灵活性高，适合非实时分析需求。

* 对象存储

  S3、GCS、Azure storage service、OSS、COS 和 OBS 等。

  应用场景：实时或近实时数据接入，支撑动态数据分析需求。

* 实时数据流

  Flink 和 Canal 等。

  应用场景：针对持续产生的数据流进行即时处理与分析，适用于需快速响应的业务场景。

根据数据来源和应用场景，您可以选择以下数据迁移工具完成迁移：

* OceanBase 官方方案
   * 官方数据迁移工具 OMS 和数据加载工具 obloader，专门大规模数据迁移设计。
   * 标准 SQL 命令 LOAD DATA 和 INSERT SQL，便于操作，适合小规模数据导入。
   * CREATE EXTERNAL TABLE 实现外部数据文件的直接查询，增加分析灵活性。
   
* 第三方集成工具
  
  Flink OceanBase Connector、DataX OceanBase Writer Plugin、Chunjun 和 Canal 等。
  
* SDK API
  
  JDBC 和 OBKV。通过标准化接口，为开发者提供灵活的数据接入方式，无论是关系型数据操作还是键值对存储，都能轻松驾驭。

本文将提供每种方案的最佳应用场景，以及如何根据数据文件格式和目标存储系统的特点来选择最适合的迁移方法。本文的目标是为您提供一个全面的迁移工具和方法框架，帮助您把数据迁移到 OceanBase 数据库，以供后续的业务分析和应用开发。

## 导入方案

本节介绍各种导入方案的特点及其应用场景。

## OMS

**基本特性：**

OceanBase 迁移服务（OceanBase Migration Service，OMS）特性如下：

- 支持多种数据源。
- 在线迁移存量数据。
- 实时同步增量数据。
- 支持多重数据校验。
- 高性能的数据迁移，安全可靠。
- 在线不停服迁移，业务应用无感知。

**适用场景：**

* OMS 适合大规模数据库到数据库的迁移。

**使用参考：**

* 关于 OMS 的详细介绍，参见 [OMS 使用文档](https://www.oceanbase.com/docs/oms-doc-cn)。

## LOAD DATA

### LOAD DATA LOCAL

**基本特性：**

LOAD DATA LOCAL 特性如下：

- 适用于本地文件导入 OceanBase 数据库。将本地文件以网络流的方式传输到 OceanBase 集群并插入。
- 适用于小数据量场景。
- 仅支持一次传输一个文件。
- 目前支持 CSV、SQL 和 Parquet 文件单导入。

**适用场景：**

* LOAD DATA LOCAL 适合大规模数据导入。

**使用参考：**

* 关于 LOAD DATA LOCAL 的使用方法，参见 [使用 LOAD DATA 语句导入数据](../../500.data-migration/700.migrate-data-from-csv-file-to-oceanbase-database/200.use-the-load-command-to-load-the-csv-data-file-to-the-oceanbase-database.md)。

### LOAD DATA FROM OSS

**基本特性：**

LOAD DATA FROM OSS 特性如下：

- 将 OSS 文件直接导入到 OceanBase 数据库中，无需通过 ODP 中转。
- 可支持一次导入多个 OSS 文件。
- 目前仅支持 CSV 文件导入。

**适用场景：**

* LOAD DATA FROM OSS 适合大规模数据导入。

**使用参考：**

* 关于 LOAD DATA FROM OSS 的操作指导，参见 [LOAD DATA（Oracle 模式）](../../700.reference/500.sql-reference/100.sql-syntax/300.common-tenant-of-oracle-mode/900.sql-statement-of-oracle-mode/300.dcl-of-oracle-mode/1900.load-data-of-oracle-mode.md)和 [LOAD DATA（MySQL 模式）](../../700.reference/500.sql-reference/100.sql-syntax/200.common-tenant-of-mysql-mode/600.sql-statement-of-mysql-mode/5900.load-data-of-mysql-mode.md)。

## obloader

**基本特性：**

obloader 特性如下：

- 支持从本地磁盘、NAS、HDFS、OSS、COS、OBS、Apache Hadoop、Aliyun OSS 和 AWS S3 导入数据库对象定义和表数据。
- 支持导入 mysqldump 导出的 INSERT SQL 格式的文件。
- 支持导入标准的 CSV、Insert SQL、Apache ORC 和 Apache Parquet 等格式的数据文件。
- 支持配置数据预处理的控制规则以及文件与表之间的字段映射关系。
- 支持导入限速、防导爆、断点恢复和错误自动重试。
- 支持指定自定义的日志目录，导入时可以保存坏数据和冲突数据。
- 支持导入工具自动对大文件进行切分且不占用额外的存储空间，无需人工切分文件。
- 支持对命令行中指定的敏感参数进行加密。包括：数据库的账号密码，云存储的账号密钥。

obloader 提供如下写入模式：

- Client 模式，支持 JDBC 或者旁路导入（DIRECT）将数据插入 OceanBase 数据库。
- Server 模式，集成 LOAD DATA 将数据导入 OceanBase 数据库。

**适用场景：**

* obloader 适合较大规模数据导入。

**使用参考：**

* 关于 obloader 的详细介绍，参见 [obloader 使用文档](https://www.oceanbase.com/docs/oceanbase-dumper-loader)。

## INSERT SQL

**适用场景：**

- INSERT INTO VALUES 适合向内部表写入少量数据。
- INSERT INTO SELECT FROM <table_name> 适合向目标表写入另外一张表（内部表或外部表）的查询结果，即表与表之间的迁移。
- INSERT /*+ [APPEND |direct(need_sort,max_error,'full')] enable_parallel_dml parallel(N) */ INTO  table_name select_sentence 旁路导入适合全量和增量旁路导入。

**使用参考：**

* 关于 INSERT SQL 的使用指导，参见 [INSERT（Oracle 模式）](../../700.reference/500.sql-reference/100.sql-syntax/300.common-tenant-of-oracle-mode/900.sql-statement-of-oracle-mode/200.dml-of-oracle-mode/200.insert-of-oracle-mode.md) 和 [INSERT（MySQL 模式）](../../700.reference/500.sql-reference/100.sql-syntax/200.common-tenant-of-mysql-mode/600.sql-statement-of-mysql-mode/5700.insert-sql-of-mysql-mode.md)。

## CREATE EXTERNAL TABLE

**适用场景：**

* 外表是数据库管理系统中的一项关键功能，通常数据库中的表存放于数据库的存储空间中，而外表的数据存储于外部存储服务中。

**使用参考：**

* 关于外表的使用指导，参见 [外表（Oracle 模式）](../../700.reference/300.database-object-management/200.manage-object-of-oracle-mode/100.manage-tables-of-oracle-mode/1000.manage-external-tables-of-oracle-mode/100.about-external-tables-of-oracle-mode.md)和 [外表（MySQL 模式）](../../700.reference/300.database-object-management/100.manage-object-of-mysql-mode/200.manage-tables-of-mysql-mode/1000.manage-external-tables-of-mysql-mode/100.about-external-tables-of-mysql-mode.md)。

## Flink

**适用场景：**

* Flink OceanBase Connector 适合从 Flink 实时导入数据。

**使用参考：**

* [使用 Flink CDC 从 MySQL 数据库同步数据到 OceanBase 数据库](../../500.data-migration/200.migrate-data-from-mysql-database-to-oceanbase-database/600.use-flink-cdc-to-migrate-data-from-mysql-database-to-oceanbase-database.md)

## Canal

**适用场景：**

* 适合从 Canal 实时导入数据。

**使用参考：**

* [使用 Canal 从 MySQL 数据库同步数据到 OceanBase 数据库](../../500.data-migration/200.migrate-data-from-mysql-database-to-oceanbase-database/500.use-canal-to-migrate-data-from-mysql-database-to-oceanbase-database.md)

## DataX

**适用场景：**

* 适合从在 MySQL、Oracle 等关系型数据库、HDFS、及 Hive 等各种数据源之间同步数据。DataX 不限制数据量，单表逐个迁移即可。迁移速度可达 60MB/S。

**使用参考：**

* [使用 DataX 迁移 MySQL 表数据到 OceanBase 数据库](../../500.data-migration/200.migrate-data-from-mysql-database-to-oceanbase-database/300.use-datax-to-migrate-data-from-mysql-database-to-oceanbase-database.md)

## CloudCanal

**适用场景：**

* 适合从 MySQL、Oracle 和 PostgreSQL 等数据库向 OceanBase 数据库 迁移或同步数据。

**使用参考：**

* [使用 CloudCanal 从 MySQL 数据库迁移数据到 OceanBase 数据库](../../500.data-migration/200.migrate-data-from-mysql-database-to-oceanbase-database/400.use-cloudcanal-to-migrate-data-from-mysql-database-to-oceanbase-database.md)

## 导入方案选择

本节介绍常见数据源支持的导入方案，旨在帮助您结合实际场景快速选择合适的导入方案。

### 对象存储

将云厂商的对象存储中的数据导入到 OceanBase 数据库的方案见下表。

|**数据源**|**支持的导入方案**|
|---|---|
| Alibaba Cloud OSS | <ul><li>obloader（<a href="https://www.oceanbase.com/docs/common-oceanbase-dumper-loader-1000000000775418#14-title-%E4%BB%8E%20Aliyun%20OSS%20%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%88%B0%20OceanBase%20%E6%95%B0%E6%8D%AE%E5%BA%93">参考文档</a>）</li><li>DataX（<a href="https://github.com/alibaba/DataX">参考文档</a>）</li><li>LOAD DATA INFILE（<a href="https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000000821997#7-title-%E4%BB%8E%20OSS%20%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE">参考文档</a>）</li><li>将数据下载到本地或者可访问的服务器上，再使用 MySQL 命令行工具或 SQL 管理工具导入到 OceanBase 数据库，或者编写脚本，使用 MySQL 的连接库执行 SQL 语句并执行批量插入。</li></ul>|
| Tencent Cloud COS | <ul><li>obloader（<a href="https://www.oceanbase.com/docs/common-oceanbase-dumper-loader-1000000000775418#14-title-%E4%BB%8E%20Aliyun%20OSS%20%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%88%B0%20OceanBase%20%E6%95%B0%E6%8D%AE%E5%BA%93">参考文档</a>）</li><li>LOAD DATA INFILE（<a href="https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000000821997#7-title-%E4%BB%8E%20OSS%20%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE">参考文档</a>）</li><li>将数据下载到本地或者可访问的服务器上，再使用 MySQL 命令行工具或 SQL 管理工具导入到 OceanBase 数据库，或者编写脚本，使用 MySQL 的连接库执行 SQL 语句并执行批量插入。</li></ul> |
| Huawei Cloud OBS | <ul><li>LOAD DATA INFILE（<a href="https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000000821997#7-title-%E4%BB%8E%20OSS%20%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE">参考文档</a>）</li><li>将数据下载到本地或者可访问的服务器上，再使用 MySQL 命令行工具或 SQL 管理工具导入到 OceanBase 数据库，或者编写脚本，使用 MySQL 的连接库执行 SQL 语句并执行批量插入。</li><li>Flink 对接 OBS（<a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/deployment/filesystems/overview/">参考文档</a>）</li></ul>|
| Amazon S3 | <ul><li>obloader（<a href="https://www.oceanbase.com/docs/common-oceanbase-dumper-loader-1000000000775418#13-title-%E4%BB%8E%20Amazon%20S3%20%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%88%B0%20OceanBase%20%E6%95%B0%E6%8D%AE%E5%BA%93">参考文档</a>）</li><li>将数据下载到本地或者可访问的服务器上，再使用 MySQL 命令行工具或 SQL 管理工具导入到 OceanBase 数据库，或者编写脚本，使用 MySQL 的连接库执行 SQL 语句并执行批量插入。</li></ul>|
| Azure Blob Storage | 将数据下载到本地或者可访问的服务器上，再使用 MySQL 命令行工具或 SQL 管理工具导入到 OceanBase 数据库，或者编写脚本，使用 MySQL 的连接库执行 SQL 语句并执行批量插入。|
| Google Cloud GCS | 将数据下载到本地或者可访问的服务器上，再使用 MySQL 命令行工具或 SQL 管理工具导入到 OceanBase 数据库，或者编写脚本，使用 MySQL 的连接库执行 SQL 语句并执行批量插入。|

### 文件系统
将本地文件系统及分布式文件系统中的数据导入到 OceanBase 数据库的方案见下表。

|**数据源**|**支持的导入方案**|
|---|---|
| Local File System (包括 NFS 和 NAS) | <ul><li>如果数据是 CSV，您可以使用 obloader（<a href="https://www.oceanbase.com/docs/common-oceanbase-dumper-loader-1000000000775418#9-title-%E5%AF%BC%E5%85%A5%20CSV%20%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6">参考文档</a>）。</li><li>如果数据是 SQL，您可以使用 LOAD DATA INFILE（<a href="https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000000821997#7-title-%E4%BB%8E%20OSS%20%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE">参考文档</a>）。</li><li>编写脚本，使用 MySQL 的连接库执行 SQL 语句并执行批量插入。</li></ul>|
| 分布式文件系统 HDFS | <ul><li>将数据导入至 TXT 文件，再使用 obloader 导入（<a href="https://www.oceanbase.com/docs/common-oceanbase-dumper-loader-1000000000775418#14-title-%E4%BB%8E%20Aliyun%20OSS%20%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%88%B0%20OceanBase%20%E6%95%B0%E6%8D%AE%E5%BA%93">参考文档</a>）。</li><li>编写自定义 ETL 脚本，使用 HDFS API 读取 HDFS 中的数据，然后使用 MySQL 的客户端库（例如 JDBC、Python 的 mysql-connector-python 等）将数据转换和导入到 OceanBase 数据库。</li></ul>|

方案步骤如下：

1. 数据评估和准备。

    * 了解数据格式：首先，了解存储在文件系统上的数据格式。数据可能以文本文件（CSV、JSON、XML 等）、二进制文件或其他格式存储。
    * 定义目标结构：确定 OceanBase 数据库的目标表结构，了解哪些数据需要迁移，是否需要转换等。

2. 数据提取。

    * 直接访问：如果 OceanBase 服务器可以直接访问文件系统的挂载点，您可以直接在 SQL 命令中使用文件路径，例如使用 LOAD DATA INFILE 命令导入 CSV 文件。
    * 数据复制：如果直接访问不可行，可能需要先将数据从文件复制到一系统 个可以直接访问 OceanBase 服务器的位置，例如本地硬盘或临时存储。

3. 数据转换（可选）。

    * 预处理：根据 OceanBase 目标表结构预处理数据。例如，你可能需要编写脚本或使用 ETL 工具来转换数据格式、清洗数据（比如去除无效或不一致的记录）、调整日期格式等。
    * 转换工具：使用如 Python、Pandas 等脚本语言或 ETL 工具执行数据转换。

4. 数据加载。

    * LOAD DATA INFILE：对于大量数据，OceanBase 的 LOAD DATA INFILE 命令是一个高效的导入选择。确保 OceanBase L 配置和权限允许从所选择的文件路径读取。
    * MySQL 客户端库：使用如 Python 的 mysql-connector-python 客户端库，通过编程方式批量插入数据。

注意事项：

* 访问权限：确保拥有NFS或NAS上数据的读取权限，以及MySQL数据库的写入权限。
* 数据安全：在迁移过程中考虑数据的安全性和隐私保护，遵守适用的数据保护法规。
* 备份：在开始任何迁移前，始终备份原始数据和目标数据库。

### 流式系统

将流式系统中的数据导入到 OceanBase 数据库的方案见下表。

|**数据源**|**支持的导入方案**|
|---|---|
|  Flink | <ul><li>MySQL CDC Connector（<a href="https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/mysql-cdc/">参考文档</a>）</li><li>Flink JDBC SQL Connector（<a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/jdbc/">参考文档</a>）</li><li>Flink OceanBase Connector（<a href="https://nightlies.apache.org/flink/flink-cdc-docs-release-3.0/docs/connectors/flink-sources/oceanbase-cdc/">参考文档</a>）</li></ul>|
| Canal | <ul><li>Canal Server（<a href="https://github.com/alibaba/canal">参考文档</a>）</li><li>Canal Adapter（<a href="https://github.com/alibaba/canal">参考文档</a>）</li></ul>|
| Spark | 使用 JDBC 接口连接到 OceanBase 数据库（[参考文档](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)）|

### 数据库

将数据库中的数据导入到 OceanBase 数据库的方案见下表。

|**数据源**|**支持的导入方案**|
|---|---|
| MySQL | <ul><li>在线迁移，使用 OMS 从数据库到数据库。</li><li>DataX。</li><li>离线迁移，把 MySQL 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。方案类似文件系统迁移。</li></ul>|
| Oracle | <ul><li>在线迁移，使用 OMS 从数据库到数据库。</li><li>DataX。</li><li>离线迁移，把 Oracle 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。</li></ul>|
| PostgreSQL | <ul><li>在线迁移，使用 OMS 从数据库到数据库。</li><li>离线迁移，把 PostgreSQL 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。</li></ul>|
| TiDB | <ul><li>在线迁移，使用 OMS 从数据库到数据库。</li><li>离线迁移，把 TiDB 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。</li></ul>|
| SQLServer | <ul><li>DataX。</li><li>离线迁移，把 SQLServer 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。</li></ul>|
| StarRocks | 离线迁移，把 StarRocks 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。|
| Doris | 离线迁移，把 Doris 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。|
| HBase |  <ul><li>DataX。</li><li>离线迁移，把 HBase 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。</li></ul>|
| MaxCompute | 离线迁移，把 MaxCompute 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。|
| Hologres | 离线迁移，把 Hologres 中的数据 dump 出来，再使用 obloader 或者 LOAD DATA。|