# 部署 NFS

在执行备份操作前，如果需要使用 NFS 软件作为备份目的地，可参考本节内容部署 NFS。

一般建议使用 OSS 或者专用的 NFS 硬件设备，可以考虑使用阿里云的 NFS 硬件设备。

## 注意事项

* 使用 NFS 环境时，需要保证先挂载 NFS，再开启备份。如果备份期间 NFS 出现问题，需要先停止数据备份和日志备份，再解决 NFS 的问题。

* 由于 OceanBase 数据库备份的并发控制需依赖 NFS4 的文件锁功能，故在挂载 NFS 时，需使用 NFS 4.1 及以上版本。

* 在使用 NFS 作为备份介质时，必须保证所有 OBServer 都挂载了同一个服务器的 NFS。同时，为保证备份的顺利进行，务必使用本文档中建议的参数挂载 NFS。挂载 NFS 具体操作请参见 [部署 NFS 客户端](#部署%20nfs%20客户端)。

* 在重启 OBServer 时，需要先启动 NFS，再启动 OBServer。

* 添加新的机器后，在启动 OBServer 前，需要保证新的机器挂载 NFS 成功或者可以备份到其他介质。

## 部署 NFS 服务器端

1. 登录 NFS 服务器。

2. 执行以下命令，通过 YUM 包管理器安装 NFS。

   ```shell
   sudo yum install nfs-utils
   ```

3. 设置 Exports。

   1. 选择一个合适的目录作为共享目录，在选择时需要考虑备份所需的空间和性能。

      例如，本文档中选择的共享目录为 `/data/nfs_server/`。

   2. 使用 `sudo vim /etc/exports` 命令打开配置文件，设置以下信息。

      ```shell
      /data/nfs_server/ 100.xx.xx.xx/16(rw,sync,all_squash)
      ```

      其中，`100.xx.xx.xx` 表示允许访问的网段。

   3. 执行以下命令，为 `nfsnobody` 赋权，确保 `nfsnobody` 有权限访问 `exports` 中指定的目录。

      ```shell
      sudo chown nfsnobody:nfsnobody -R /data/nfs_server
      ```

4. 配置 NFS 参数。

   1. 执行 `sudo vim /etc/sysconfig/nfs` 命令，打开配置文件。

   2. 调整如下所示参数：

      ```shell
      RPCNFSDCOUNT=8
      RPCNFSDARGS="-N 2 -N 3 -U"
      NFSD_V4_GRACE=90
      NFSD_V4_LEASE=90
      ```

   3. 执行以下命令，重新启动 NFS。

      ```shell
      sudo systemctl restart nfs-config

      sudo systemctl restart nfs-server
      ```

5. 设置 Slot Table。

   1. 执行 `sudo vim /etc/sysctl.conf` 命令，打开 `sysctl.conf`配置文件，在文件中添加一行如下信息：

      ```xml
      sunrpc.tcp_max_slot_table_entries=128
      ```

   2. 执行以下命令，将同时发起的 NFS 请求数量修改为 128。

      ```sql
      sysctl -w sunrpc.tcp_max_slot_table_entries=128
      ```

      命令执行成功后，可以通过 `cat /proc/sys/sunrpc/tcp_max_slot_table_entries` 命令查看设置是否生效，如果返回值为 128，则说明修改成功。

   3. （可选）重启机器。

## 部署 NFS 客户端

部署 NFS 客户端时，需要在所有 OBServer 机器上进行操作。

以下以在某一台 OBServer 上的操作为例，提供操作指导。

1. 登录 OBServer。

2. 执行以下命令，通过 YUM 包管理器安装 NFS。

   ```shell
   sudo yum install nfs-utils
   ```

3. 设置 Slot Table。

   1. 执行 `sudo vim /etc/sysctl.conf` 命令，打开`sysctl.conf` 配置文件，在文件中添加一行如下信息：

      ```xml
      sunrpc.tcp_max_slot_table_entries=128
      ```

   2. 执行以下命令，将同时发起的 NFS 请求数量修改为 128。

      ```sql
      sysctl -w sunrpc.tcp_max_slot_table_entries=128
      ```

      命令执行成功后，可以通过 `cat /proc/sys/sunrpc/tcp_max_slot_table_entries` 命令查看设置是否生效，如果返回值为 128，则说明修改成功。

   3. （可选）重启机器。

4. 选择一个合适的目录作为 Mount 点， 执行以下命令，挂载 NFS。

   例如，本文档中假设 Mount 到 `/data/nfs` 目录，如果没有合适的目录可以新建一个目录。

   ```shell
   sudo mount -tnfs4 -o rw,nfsvers=4.1,sync,lookupcache=positive,hard,timeo=600,wsize=1048576,rsize=1048576,namlen=255 100.xx.xx.xx:/data/nfs_server /data/nfs
   ```

   语句中：

   * `nfsvers=4.1`：由于备份依赖 nfs 4 原生的文件锁，建议使用 nfs 4.1 及以上版本。nfs 4.0 有一个已知 Bug，在重命名文件以后可能会读到旧文件。

   * `sync`：使用同步写保证数据能及时刷到服务端，从而保证数据的一致性。

   * `lookupcache=positive`：用于避免并发访问目录或者文件时误报目录或文件不存在的问题，保证数据的一致性。

   * `hard`：在 NFS 不可用的情况下，系统会卡住应用的读写请求，以保证数据的一致性。不能使用 `soft` 选项，会有数据错误的风险。

   * `timeo`：用于指定重试的等待时间，单位为 0.1s。在设置时，建议不要设置得过大，建议值为 `600`。

   * `wsize`：表示读的数据块大小，建议设置为 `1048576`。

   * `rsize`：表示写的数据库大小，建议设置为 `1048576`。

   * `namlen`：建议设置为 `255`。

   * `100.xx.xx.xx`：表示 NFS 服务器的 IP 地址。

   >**注意**
   >
   >* 在挂载 NFS 时，必须保证备份挂载环境的参数中包含 `nfsvers=4.1`、`sync`、`lookupcache=positive` 及 `hard`。
   >* Docker 环境需要在宿主机上挂载 NFS，然后映射到 Docker 里面。如果 Docker 内部直接挂载 NFS，可能会出现客户端 Hung 住的场景。

5. 挂载完成后，可执行以下命令，验证 NFS 的性能。

   ```shell
   fio -filename=/data/nfs/fio_test -direct=1  -rw=randwrite -bs=2048K -size=100G  -runtime=300 -group_reporting -name=mytest  -ioengine=libaio -numjobs=1 -iodepth=64 -iodepth_batch=8 -iodepth_low=8 -iodepth_batch_complete=8
   ```

   例如，执行结果如下：

   ```shell
   Run status group 0 (all jobs):
   WRITE: io=322240MB, aggrb=1074.2MB/s, minb=1074.2MB/s, maxb=1074.2MB/s, mint=300006msec, maxt=300006msec
   ```
